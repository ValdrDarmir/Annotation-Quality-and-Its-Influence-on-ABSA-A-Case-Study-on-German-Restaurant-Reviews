{
    "model_name": "deepset/gbert-base",
    "task": "acsa",
    "data_setting": "orig",
    "dataset": "llm/acsa",
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-05,
    "num_train_epochs": 40.0,
    "eval_type": "test",
    "train_runtime": 635.8643662929535,
    "gpu_util": 3.174
}