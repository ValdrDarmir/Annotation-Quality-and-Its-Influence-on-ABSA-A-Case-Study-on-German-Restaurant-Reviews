{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef86023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>f1-micro</th>\n",
       "      <th>f1-macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acsa</td>\n",
       "      <td>0.8505</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>0.7398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acsa</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.8430</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acsa</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.8546</td>\n",
       "      <td>0.7641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acsa</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>0.8441</td>\n",
       "      <td>0.7494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acsa</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>0.7536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task  f1-micro  f1-macro  accuracy\n",
       "0  acsa    0.8505    0.8413    0.7398\n",
       "1  acsa    0.8525    0.8430    0.7430\n",
       "2  acsa    0.8663    0.8546    0.7641\n",
       "3  acsa    0.8568    0.8441    0.7494\n",
       "4  acsa    0.8595    0.8337    0.7536"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "task = \"acsa\"\n",
    "RESULTS_PATH = 'Students/'  # or 'LLM/'\n",
    "runs = []\n",
    "\n",
    "# loop through subfolders in RESULTS_PATH\n",
    "folder_names = [\n",
    "    folder for folder in os.listdir(RESULTS_PATH)\n",
    "    if os.path.isdir(os.path.join(RESULTS_PATH, folder))\n",
    "    and folder != '.ipynb_checkpoints'\n",
    "]\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    try:\n",
    "        cond_parameters = folder_name.split('_')  # if you still want to keep some info from folder name\n",
    "        # pick correct metrics file depending on task\n",
    "        if folder_name.startswith('acd'):\n",
    "            df = pd.read_csv(os.path.join(RESULTS_PATH, folder_name, 'metrics_asp.tsv'), sep='\\t')\n",
    "        elif folder_name.startswith('tasd'):\n",
    "            df = pd.read_csv(os.path.join(RESULTS_PATH, folder_name, 'metrics_phrases.tsv'), sep='\\t')\n",
    "        else:  # acsa\n",
    "            df = pd.read_csv(os.path.join(RESULTS_PATH, folder_name, 'result_asp_pol.tsv'), sep='\\t')\n",
    "\n",
    "        df = df.set_index(df.columns[0])\n",
    "\n",
    "        if task == 'tasd':\n",
    "            f1_micro = df.loc['Micro-AVG', 'f1']\n",
    "            f1_macro = df.loc['Macro-AVG', 'f1']\n",
    "            accuracy = df.loc['Micro-AVG', 'accuracy']\n",
    "        elif task == 'acsa':  # acsa\n",
    "            f1_micro = df.loc['Micro-AVG', 'F1']\n",
    "            f1_macro = df.loc['Macro-AVG', 'F1']\n",
    "            accuracy = df.loc['Micro-AVG', 'Accuracy']\n",
    "\n",
    "        runs.append([cond_parameters[0], f1_micro, f1_macro, accuracy])\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# build DataFrame with only the columns you need\n",
    "results_all = pd.DataFrame(runs, columns=[\"task\", \"f1-micro\", \"f1-macro\", \"accuracy\"])\n",
    "\n",
    "print(RESULTS_PATH)\n",
    "results_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657bbcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across seeds:\n",
      "Students/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>f1-micro</th>\n",
       "      <th>f1-macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acsa</td>\n",
       "      <td>85.71</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task  f1-micro  f1-macro  accuracy\n",
       "0  acsa     85.71     84.33      0.75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compute average across all runs for this task ---\n",
    "results_avg = results_all.groupby(\"task\")[[\"f1-micro\", \"f1-macro\", \"accuracy\"]].mean().reset_index()\n",
    "\n",
    "# Convert to percentages for F1 if you want (optional)\n",
    "results_avg[\"f1-micro\"] = (results_avg[\"f1-micro\"] * 100).round(2)\n",
    "results_avg[\"f1-macro\"] = (results_avg[\"f1-macro\"] * 100).round(2)\n",
    "results_avg[\"accuracy\"] = results_avg[\"accuracy\"].round(4)\n",
    "\n",
    "print(\"\\nAverage results across seeds:\")\n",
    "print(RESULTS_PATH)\n",
    "results_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c21c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw results for TASD ===\n",
      "   task   dataset eval_type data_setting learning-rate batch_size epochs seed  \\\n",
      "0  tasd  llm-tasd      test       orig-o        0.0002         16      6   10   \n",
      "1  tasd  llm-tasd      test       orig-o        0.0002         16      6   15   \n",
      "2  tasd  llm-tasd      test       orig-o        0.0002         16      6   20   \n",
      "3  tasd  llm-tasd      test       orig-o        0.0002         16      6   25   \n",
      "4  tasd  llm-tasd      test       orig-o        0.0002         16      6    5   \n",
      "\n",
      "    training_time  used_memory  \n",
      "0  0:38:51.495268        14.27  \n",
      "1  0:38:49.898244        14.27  \n",
      "2  0:38:50.326308        14.27  \n",
      "3  0:38:48.677868        14.27  \n",
      "4  0:38:45.024006        14.27  \n",
      "=== Grouped results for TASD ===\n",
      "  data_setting eval_type learning-rate epochs batch_size  training_time_sec  \\\n",
      "0       orig-o      test        0.0002      6         16        2329.084339   \n",
      "\n",
      "   used_memory training_time  \n",
      "0        14.27         00:38  \n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- SWITCH HERE ----\n",
    "task_mode = \"tasd\"   # change to \"acsa\" when needed\n",
    "# ---------------------\n",
    "\n",
    "col_names = [\n",
    "    'task', 'dataset', 'eval_type', 'data_setting',\n",
    "    'learning-rate', 'batch_size', 'epochs', 'seed',\n",
    "    'training_time', 'used_memory'\n",
    "]\n",
    "runs = []\n",
    "\n",
    "RESULTS_PATH = 'LLM/'\n",
    "folder_names = [\n",
    "    folder for folder in os.listdir(RESULTS_PATH) \n",
    "    if os.path.isdir(os.path.join(RESULTS_PATH, folder)) \n",
    "    and folder != '.ipynb_checkpoints'\n",
    "]\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    try:\n",
    "        cond_parameters = folder_name.split('_')\n",
    "\n",
    "        # Only process folders matching the chosen task\n",
    "        if cond_parameters[0] != task_mode:\n",
    "            continue  \n",
    "\n",
    "        config_path = os.path.join(RESULTS_PATH, folder_name, 'config.json')\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Add runtime + GPU memory\n",
    "        cond_parameters.append(config.get(\"training_time\", None))\n",
    "        cond_parameters.append(config.get(\"used_memory\", None))\n",
    "\n",
    "        runs.append(cond_parameters)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {folder_name}: {e}\")\n",
    "        pass\n",
    "\n",
    "# Build DataFrame\n",
    "results_all = pd.DataFrame(runs, columns=col_names)\n",
    "\n",
    "print(f\"=== Raw results for {task_mode.upper()} ===\")\n",
    "print(results_all.head())\n",
    "\n",
    "# ---- Helper: convert \"HH:MM:SS.sss\" → seconds\n",
    "def parse_time_to_seconds(t):\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    try:\n",
    "        h, m, s = t.split(\":\")\n",
    "        return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "results_all[\"training_time_sec\"] = results_all[\"training_time\"].apply(parse_time_to_seconds)\n",
    "\n",
    "# Group configs\n",
    "config_cols = [\"data_setting\", \"eval_type\", \"learning-rate\", \"epochs\", \"batch_size\"]\n",
    "\n",
    "df_grouped = (\n",
    "    results_all.groupby(config_cols).agg({\n",
    "        \"training_time_sec\": \"mean\",\n",
    "        \"used_memory\": \"mean\"\n",
    "    }).reset_index()\n",
    ")\n",
    "\n",
    "# Format runtime back into HH:MM\n",
    "df_grouped[\"training_time\"] = df_grouped[\"training_time_sec\"].apply(\n",
    "    lambda x: f\"{int(x//3600):02d}:{int((x%3600)//60):02d}\" if not np.isnan(x) else None\n",
    ")\n",
    "\n",
    "print(f\"=== Grouped results for {task_mode.upper()} ===\")\n",
    "print(df_grouped.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
