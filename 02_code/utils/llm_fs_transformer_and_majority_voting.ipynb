{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f8df25",
   "metadata": {},
   "source": [
    "### Transform raw output in simple json and jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a54ad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: acsa_students_redo_llm_fs\\acsa_students_gemma3_27b_10_50.json\n",
      "Processing file: acsa_students_redo_llm_fs\\acsa_students_gemma3_27b_15_50.json\n",
      "Processing file: acsa_students_redo_llm_fs\\acsa_students_gemma3_27b_20_50.json\n",
      "Processing file: acsa_students_redo_llm_fs\\acsa_students_gemma3_27b_25_50.json\n",
      "Processing file: acsa_students_redo_llm_fs\\acsa_students_gemma3_27b_5_50.json\n",
      "Processing complete. Cleaned files are in 'results_clean/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Input and output base directories\n",
    "base_dir = 'acsa_students_redo_llm_fs'\n",
    "output_dir = 'acsa_students_redo_llm_fs_transform'\n",
    "\n",
    "# Walk through all files in directory\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load JSON data\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to load {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Remove all keys except selected ones\n",
    "            cleaned_data = []\n",
    "            for obj in data:\n",
    "                cleaned_obj = {\n",
    "                    'id': obj.get('id'),\n",
    "                    'original_id': obj.get('original_id'),\n",
    "                    'text': obj.get('text'),\n",
    "                    'pred_label': obj.get('pred_label')\n",
    "                }\n",
    "                cleaned_data.append(cleaned_obj)\n",
    "\n",
    "            # Construct output path, preserving subfolder structure\n",
    "            rel_path = os.path.relpath(root, base_dir)  # relative to base_dir\n",
    "            target_dir = os.path.join(output_dir, rel_path)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            out_file_path = os.path.join(target_dir, file)\n",
    "\n",
    "            # Save cleaned JSON\n",
    "            with open(out_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Also save as JSONL with \"labels\" format\n",
    "            out_file_path_jsonl = os.path.splitext(out_file_path)[0] + \".jsonl\"\n",
    "            with open(out_file_path_jsonl, 'w', encoding='utf-8') as f_jsonl:\n",
    "                for obj in cleaned_data:\n",
    "                    jsonl_entry = {\n",
    "                        \"id\": obj[\"id\"],\n",
    "                        \"original_id\": obj[\"original_id\"],\n",
    "                        \"text\": obj[\"text\"],\n",
    "                        # wrap pred_label into [[\"category\", \"polarity\", \"term\"]]\n",
    "                        \"labels\": obj.get(\"pred_label\")\n",
    "       \n",
    "                    }\n",
    "                    f_jsonl.write(json.dumps(jsonl_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Processing complete. Cleaned files are in 'results_clean/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac905d5",
   "metadata": {},
   "source": [
    "### TASD: Merge the 5 seeds with majority vote into a single file (separated by splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total excluded aspects in Split 0: 104\n",
      "Merged results saved to results_majority\\tasd_crowd_gemma3_27b_merged.jsonl\n",
      "Total excluded aspects in Split 1: 104\n",
      "Merged results saved to results_majority\\tasd_crowd_gemma3_27b_merged.jsonl\n",
      "Total excluded aspects in Split 2: 104\n",
      "Merged results saved to results_majority\\tasd_crowd_gemma3_27b_merged.jsonl\n",
      "Total excluded aspects in Split 3: 104\n",
      "Merged results saved to results_majority\\tasd_crowd_gemma3_27b_merged.jsonl\n",
      "Total excluded aspects in Split 4: 104\n",
      "Merged results saved to results_majority\\tasd_crowd_gemma3_27b_merged.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# --- Helper functions ---\n",
    "def get_frequency_for_counts(counts, minimum):\n",
    "    \"\"\"Return the frequency count based on minimum appearance across splits.\"\"\"\n",
    "    return sorted(counts, reverse=True)[0:minimum][minimum-1]\n",
    "\n",
    "def get_unique_keys(dict_list):\n",
    "    \"\"\"Return all unique keys across a list of dictionaries.\"\"\"\n",
    "    unique_keys = set()\n",
    "    for d in dict_list:\n",
    "        unique_keys.update(d.keys())\n",
    "    return list(unique_keys)\n",
    "\n",
    "def merge_aspect_lists(aspect_lists, minimum_appearance=3, oid=None):\n",
    "    \"\"\"\n",
    "    Merge multiple aspect lists based on a minimum appearance, \n",
    "    with warning for low-frequency aspects.\n",
    "    \"\"\"\n",
    "    counter_exclude = 0\n",
    "    aspect_lists_counter = []\n",
    "    for aspect_list in aspect_lists:\n",
    "        aspect_counter = dict(Counter([\"#####\".join(aspect) for aspect in aspect_list]))\n",
    "        aspect_lists_counter.append(aspect_counter)\n",
    "        \n",
    "    unique_tuples = get_unique_keys(aspect_lists_counter)\n",
    "\n",
    "    label = []\n",
    "    for tuple_str in unique_tuples:\n",
    "        counts = [asp.get(tuple_str, 0) for asp in aspect_lists_counter]\n",
    "        total_count = sum(counts)\n",
    "        \n",
    "        if total_count < minimum_appearance:\n",
    "            counter_exclude += 1\n",
    "            #print(f\"Warning: original_id={oid}, aspect={tuple_str.split('#####')} appeared only {total_count} times\")\n",
    "        \n",
    "        count_tuple = get_frequency_for_counts(counts, minimum_appearance)\n",
    "        tuple_reverse = tuple(tuple_str.split(\"#####\"))\n",
    "        label += count_tuple * [tuple_reverse]\n",
    "        \n",
    "    return label, counter_exclude\n",
    "\n",
    "# --- Paths ---\n",
    "input_folder = \"results_clean\"\n",
    "output_folder = \"results_majority\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for k in range(5):\n",
    "    splits = [f\"tasd_gerestaurant_gemma3_27b_{i}_30_Split_{k+1}.jsonl\" for i in range(5)]\n",
    "\n",
    "    # --- Merge data by 'original_id' ---\n",
    "    data_by_id = {}\n",
    "    for split_file in splits:\n",
    "        with open(os.path.join(input_folder, split_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                oid = entry[\"original_id\"]\n",
    "                if oid not in data_by_id:\n",
    "                    data_by_id[oid] = {\n",
    "                        \"id\": entry[\"id\"],\n",
    "                        \"original_id\": oid,\n",
    "                        \"text\": entry[\"text\"],\n",
    "                        \"labels_lists\": []\n",
    "                    }\n",
    "                data_by_id[oid][\"labels_lists\"].append(entry.get(\"labels\", []))\n",
    "\n",
    "    # --- Apply majority merge ---\n",
    "    merged_entries = []\n",
    "    total_excluded = 0\n",
    "    for oid, info in data_by_id.items():\n",
    "        merged_labels, counter_exclude = merge_aspect_lists(info[\"labels_lists\"], minimum_appearance=3, oid=oid)\n",
    "        total_excluded += counter_exclude\n",
    "        merged_entries.append({\n",
    "            \"id\": info[\"id\"],\n",
    "            \"original_id\": info[\"original_id\"],\n",
    "            \"text\": info[\"text\"],\n",
    "            \"labels\": merged_labels\n",
    "        })\n",
    "\n",
    "    print(f\"Total excluded aspects in Split {k}: {total_excluded}\")\n",
    "\n",
    "    # --- Save merged result ---\n",
    "    output_file = os.path.join(output_folder, f\"tasd_gerestaurant_gemma3_27b_merged_30_Split_{k}.jsonl\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in merged_entries:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Merged results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c9536",
   "metadata": {},
   "source": [
    "### ACSA: Remove duplicates in lines ([[\"Essen\", \"Positiv\"], [\"Essen\", \"Positiv\"]] -> [[\"Essen\", \"Positiv\"]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7f1c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved: ../../11_annotations/x/updatet_labels_original_acsa.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# --- Paths ---\n",
    "input_folder = \"../../11_annotations/t/\"\n",
    "output_folder = \"../../11_annotations/x/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def clean_labels(entry, field_name):\n",
    "    \"\"\"Remove duplicate tuples and check Konflikt polarity.\"\"\"\n",
    "    labels = entry.get(field_name, [])\n",
    "    unique_labels = list({tuple(label) for label in labels})\n",
    "    entry[field_name] = unique_labels\n",
    "\n",
    "    if any(label[1] == \"Konflikt\" for label in unique_labels):\n",
    "        print(f\"⚠️ Konflikt found in id={entry.get('id')} (original_id={entry.get('original_id')})\")\n",
    "\n",
    "    return entry\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not (filename.endswith(\".jsonl\") or filename.endswith(\".json\")):\n",
    "        continue\n",
    "\n",
    "    input_file = os.path.join(input_folder, filename)\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "\n",
    "    cleaned_entries = []\n",
    "\n",
    "    # Handle JSONL\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "\n",
    "                if \"labels\" in entry:\n",
    "                    entry = clean_labels(entry, \"labels\")\n",
    "                if \"pred_label\" in entry:\n",
    "                    entry = clean_labels(entry, \"pred_label\")\n",
    "\n",
    "                cleaned_entries.append(entry)\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for entry in cleaned_entries:\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Handle JSON\n",
    "    elif filename.endswith(\".json\"):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # If file contains a list of entries\n",
    "        if isinstance(data, list):\n",
    "            for entry in data:\n",
    "                if \"labels\" in entry:\n",
    "                    entry = clean_labels(entry, \"labels\")\n",
    "                if \"pred_label\" in entry:\n",
    "                    entry = clean_labels(entry, \"pred_label\")\n",
    "            cleaned_entries = data\n",
    "\n",
    "        # If file contains a dict with keys\n",
    "        elif isinstance(data, dict):\n",
    "            if \"labels\" in data:\n",
    "                data = clean_labels(data, \"labels\")\n",
    "            if \"pred_label\" in data:\n",
    "                data = clean_labels(data, \"pred_label\")\n",
    "            cleaned_entries = data\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_entries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Cleaned file saved: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b750f5",
   "metadata": {},
   "source": [
    "### ACSA: Merge the 5 seeds with majority vote into a single file (separated by splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b032a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total excluded labels in Split 0: 10\n",
      "Merged results saved to results_majority\\acsa_gerestaurant_gemma3_27b_merged_30_Split_0.jsonl\n",
      "Total excluded labels in Split 1: 15\n",
      "Merged results saved to results_majority\\acsa_gerestaurant_gemma3_27b_merged_30_Split_1.jsonl\n",
      "Total excluded labels in Split 2: 17\n",
      "Merged results saved to results_majority\\acsa_gerestaurant_gemma3_27b_merged_30_Split_2.jsonl\n",
      "Total excluded labels in Split 3: 13\n",
      "Merged results saved to results_majority\\acsa_gerestaurant_gemma3_27b_merged_30_Split_3.jsonl\n",
      "Total excluded labels in Split 4: 8\n",
      "Merged results saved to results_majority\\acsa_gerestaurant_gemma3_27b_merged_30_Split_4.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# --- Helper functions ---\n",
    "def get_frequency_for_counts(counts, minimum):\n",
    "    \"\"\"Return the frequency count based on minimum appearance across splits.\"\"\"\n",
    "    return sorted(counts, reverse=True)[0:minimum][minimum-1]\n",
    "\n",
    "def get_unique_keys(dict_list):\n",
    "    \"\"\"Return all unique keys across a list of dictionaries.\"\"\"\n",
    "    unique_keys = set()\n",
    "    for d in dict_list:\n",
    "        unique_keys.update(d.keys())\n",
    "    return list(unique_keys)\n",
    "\n",
    "def merge_aspect_lists(aspect_lists, minimum_appearance=3, oid=None):\n",
    "    \"\"\"\n",
    "    Merge multiple aspect (category, polarity) tuples based on a minimum appearance,\n",
    "    with warning for low-frequency labels.\n",
    "    \"\"\"\n",
    "    counter_exclude = 0\n",
    "    aspect_lists_counter = []\n",
    "    for aspect_list in aspect_lists:\n",
    "        # Here aspect_list contains tuples: (category, polarity)\n",
    "        aspect_counter = dict(Counter([\"#####\".join(aspect) for aspect in aspect_list]))\n",
    "        aspect_lists_counter.append(aspect_counter)\n",
    "        \n",
    "    unique_tuples = get_unique_keys(aspect_lists_counter)\n",
    "\n",
    "    label = []\n",
    "    for tuple_str in unique_tuples:\n",
    "        counts = [asp.get(tuple_str, 0) for asp in aspect_lists_counter]\n",
    "        total_count = sum(counts)\n",
    "        \n",
    "        if total_count < minimum_appearance:\n",
    "            counter_exclude += 1\n",
    "            # print(f\"Warning: original_id={oid}, label={tuple_str.split('#####')} appeared only {total_count} times\")\n",
    "        \n",
    "        count_tuple = get_frequency_for_counts(counts, minimum_appearance)\n",
    "        tuple_reverse = tuple(tuple_str.split(\"#####\"))  # now (category, polarity)\n",
    "        label += count_tuple * [tuple_reverse]\n",
    "        \n",
    "    return label, counter_exclude\n",
    "\n",
    "# --- Paths ---\n",
    "input_folder = \"results_clean/acsa_clean\"\n",
    "output_folder = \"results_majority\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for k in range(5):\n",
    "    splits = [f\"acsa_gerestaurant_gemma3_27b_{i}_30_Split_{k+1}.jsonl\" for i in range(5)]\n",
    "\n",
    "    # --- Merge data by 'original_id' ---\n",
    "    data_by_id = {}\n",
    "    for split_file in splits:\n",
    "        with open(os.path.join(input_folder, split_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                oid = entry[\"original_id\"]\n",
    "                if oid not in data_by_id:\n",
    "                    data_by_id[oid] = {\n",
    "                        \"id\": entry[\"id\"],\n",
    "                        \"original_id\": oid,\n",
    "                        \"text\": entry[\"text\"],\n",
    "                        \"labels_lists\": []\n",
    "                    }\n",
    "                data_by_id[oid][\"labels_lists\"].append(entry.get(\"labels\", []))\n",
    "\n",
    "    # --- Apply majority merge ---\n",
    "    merged_entries = []\n",
    "    total_excluded = 0\n",
    "    for oid, info in data_by_id.items():\n",
    "        merged_labels, counter_exclude = merge_aspect_lists(info[\"labels_lists\"], minimum_appearance=3, oid=oid)\n",
    "        total_excluded += counter_exclude\n",
    "        merged_entries.append({\n",
    "            \"id\": info[\"id\"],\n",
    "            \"original_id\": info[\"original_id\"],\n",
    "            \"text\": info[\"text\"],\n",
    "            \"labels\": merged_labels\n",
    "        })\n",
    "\n",
    "    print(f\"Total excluded labels in Split {k}: {total_excluded}\")\n",
    "\n",
    "    # --- Save merged result ---\n",
    "    output_file = os.path.join(output_folder, f\"acsa_gerestaurant_gemma3_27b_merged_30_Split_{k}.jsonl\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in merged_entries:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Merged results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5373b2",
   "metadata": {},
   "source": [
    "### Combine the splits to a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 4520 entries from 5 files into results_clean\\tasd_crowd.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# --- Paths ---\n",
    "input_folder = \"results_majority\"   # Folder containing all JSONL files\n",
    "output_folder = \"results_majority\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"acsa_gerestaurant_gemma3_27b_merged_30.jsonl\")\n",
    "\n",
    "# --- List all .jsonl files in the input folder ---\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Output file already exists: {output_file}. Aborting to prevent overwrite.\")\n",
    "else:\n",
    "    jsonl_files = [f for f in os.listdir(input_folder) if f.endswith(\".jsonl\")]\n",
    "\n",
    "    combined_count = 0\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for file_name in jsonl_files:\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as in_f:\n",
    "                for line in in_f:\n",
    "                    try:\n",
    "                        entry = json.loads(line.strip())\n",
    "                        out_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                        combined_count += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid JSON line in {file_name}\")\n",
    "\n",
    "    print(f\"Combined {combined_count} entries from {len(jsonl_files)} files into {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
