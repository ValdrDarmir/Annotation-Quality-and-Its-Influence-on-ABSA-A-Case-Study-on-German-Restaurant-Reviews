{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a8f958",
   "metadata": {},
   "source": [
    "### Add original_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "066973b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Enriched JSONL saved as ../../11_annotations/gt_comparison/out.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input/output file paths\n",
    "jsonl_file = \"../../11_annotations/gt_comparison/gt_100_comparison.jsonl\"\n",
    "json_file = \"../../11_annotations/gt_comparison/gt_100_comparison.json\"\n",
    "output_file = \"../../11_annotations/gt_comparison/out.jsonl\"\n",
    "\n",
    "# Load the mapping JSON\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    mapping_data = json.load(f)\n",
    "\n",
    "# Build a dictionary: text -> original_id (forced as string)\n",
    "text_to_id = {}\n",
    "\n",
    "if isinstance(mapping_data, dict):\n",
    "    for number, content in mapping_data.items():\n",
    "        data = content.get(\"data\", {})\n",
    "        text = data.get(\"text\")\n",
    "        original_id = data.get(\"original_id\")\n",
    "        if text and original_id is not None:\n",
    "            text_to_id[text] = str(original_id).strip()\n",
    "elif isinstance(mapping_data, list):\n",
    "    for content in mapping_data:\n",
    "        data = content.get(\"data\", {})\n",
    "        text = data.get(\"text\")\n",
    "        original_id = data.get(\"original_id\")\n",
    "        if text and original_id is not None:\n",
    "            text_to_id[text] = str(original_id).strip()\n",
    "\n",
    "# Process JSONL and enrich\n",
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        entry = json.loads(line)\n",
    "        text = entry.get(\"text\")\n",
    "        orig_id = text_to_id.get(text, None)\n",
    "\n",
    "        new_entry = {}\n",
    "        keys = list(entry.keys())\n",
    "        if keys:\n",
    "            # always convert first key to string if it is \"id\"\n",
    "            first_key = keys[0]\n",
    "            if first_key == \"id\":\n",
    "                new_entry[first_key] = str(entry[first_key])\n",
    "            else:\n",
    "                new_entry[first_key] = entry[first_key]\n",
    "\n",
    "            # add original_id as string\n",
    "            new_entry[\"original_id\"] = str(orig_id) if orig_id is not None else None\n",
    "\n",
    "            for k in keys[1:]:\n",
    "                # skip if we already added original_id\n",
    "                if k == \"original_id\":\n",
    "                    continue\n",
    "                new_entry[k] = entry[k]\n",
    "        else:\n",
    "            new_entry[\"original_id\"] = str(orig_id) if orig_id is not None else None\n",
    "\n",
    "        outfile.write(json.dumps(new_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"✅ Done! Enriched JSONL saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8ca32",
   "metadata": {},
   "source": [
    "### Remove aspect phrase for acsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d276629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"../../11_annotations/ground_truth/tasd_testset.jsonl\"\n",
    "output_path = \"../../11_annotations/ground_truth/acsa_testset.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # Truncate labels to max length 2\n",
    "        new_labels = []\n",
    "        for label in record[\"labels\"]:\n",
    "            if len(label) > 2:\n",
    "                new_labels.append(label[:2])\n",
    "            else:\n",
    "                new_labels.append(label)\n",
    "        \n",
    "        # Remove duplicates in the same line\n",
    "        unique_labels = []\n",
    "        seen = set()\n",
    "        for label in new_labels:\n",
    "            tup = tuple(label)  # convert list to tuple to make it hashable\n",
    "            if tup not in seen:\n",
    "                seen.add(tup)\n",
    "                unique_labels.append(label)\n",
    "        \n",
    "        record[\"labels\"] = unique_labels\n",
    "        outfile.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cf5e5",
   "metadata": {},
   "source": [
    "### Rename categories and check for conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159ce7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Mapping from German to English\n",
    "POLARITY_MAP = {\n",
    "    \"Positiv\": \"positive\",\n",
    "    \"Negativ\": \"negative\",\n",
    "    \"Neutral\": \"neutral\",\n",
    "    \"Konflikt\": \"conflict\"\n",
    "}\n",
    "\n",
    "def transform_labels(sample):\n",
    "    \"\"\"Transform one sample's labels.\"\"\"\n",
    "    new_labels = []\n",
    "    for label in sample.get(\"labels\", []):\n",
    "        category = label[0].lower()\n",
    "        polarity = POLARITY_MAP.get(label[1], label[1]).lower()\n",
    "        rest = label[2:] if len(label) > 2 else []\n",
    "        new_labels.append([category, polarity] + rest)\n",
    "    sample[\"labels\"] = new_labels\n",
    "    return sample\n",
    "\n",
    "def process_jsonl(input_path, output_path):\n",
    "    \"\"\"Load JSONL, transform labels, and save to new JSONL.\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            sample = json.loads(line)\n",
    "            transformed = transform_labels(sample)\n",
    "            outfile.write(json.dumps(transformed, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "input = \"../../11_annotations/few_shot/tasd/trainset_llm.jsonl\"\n",
    "output = \"../../11_annotations/few_shot/tasd/trainset_llm_low.jsonl\"\n",
    "process_jsonl(input, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb6ffc",
   "metadata": {},
   "source": [
    "### Check for mistakes (Duplicates in Labels -> ACSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110667cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Path to file ---\n",
    "# input_file = \"../../11_annotations/few_shot/acsa/acsa_llm_trainset.jsonl\"\n",
    "# input_file = \"../../11_annotations/ground_truth/acsa_testset.jsonl\"\n",
    "input_file = \"../../11_annotations/crowd/tasd/trainset_crowd_noconflict.jsonl\"\n",
    "\n",
    "# --- Validation ---\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        eid = entry.get(\"id\")\n",
    "        labels = entry.get(\"labels\", [])\n",
    "\n",
    "        # --- Check for Konflikt/conflict ---\n",
    "        if any(label[1].lower() == \"konflikt\" or label[1].lower() == \"conflict\" for label in labels):\n",
    "            print(f\"⚠️ Konflikt/Conflict found in id={eid}\")\n",
    "\n",
    "        # --- Check for duplicates ---\n",
    "        seen = set()\n",
    "        duplicates = set()\n",
    "        for label in labels:\n",
    "            tup = tuple(label)\n",
    "            if tup in seen:\n",
    "                duplicates.add(tup)\n",
    "            else:\n",
    "                seen.add(tup)\n",
    "\n",
    "        if duplicates:\n",
    "            print(f\"⚠️ Duplicate labels in id={eid}: {duplicates}\")\n",
    "    print(\"✅ Validation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4b861",
   "metadata": {},
   "source": [
    "## Transform csv to jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Input CSV and output JSONL file paths\n",
    "csv_file = \"../../11_annotations/text_only/trainset_1000.csv\"\n",
    "jsonl_file = \"../../11_annotations/text_only/trainset_1000.jsonl\"\n",
    "\n",
    "# Open CSV and JSONL\n",
    "with open(csv_file, newline='', encoding='utf-8') as f_csv, open(jsonl_file, 'w', encoding='utf-8') as f_jsonl:\n",
    "    reader = csv.DictReader(f_csv)  # Reads CSV into dictionaries\n",
    "    for row in reader:\n",
    "        # Convert each row to JSON and write as one line\n",
    "        json.dump(row, f_jsonl, ensure_ascii=False)\n",
    "        f_jsonl.write(\"\\n\")\n",
    "\n",
    "print(f\"CSV '{csv_file}' has been converted to JSONL '{jsonl_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cea46",
   "metadata": {},
   "source": [
    "## Display Aspect Phrases with special Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d01ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 special aspect phrases:\n",
      "Bedienung, Susanna\n",
      "Almhütten-Burger\n",
      "Preis-Leistungs-Verhältnis\n",
      "Fischplatte (für 1 Person\n",
      "Preis- Leistung\n",
      "Döner & Co angesiedelten Küche\n",
      "Varieté Show\n",
      "Mitarbeiter/innen\n",
      "Preis-Leistungsverhältnis\n",
      "Mezzelune ( schwarz\n",
      "Araz (Kellner\n",
      "Etablissement „Augustiner\n",
      "Tapas-Auswahl\n",
      "Gruppen-Events\n",
      "RESTAURANT_NAME\n",
      "Hauptgang, Rinderfilet\n",
      "Soßen-Auswahl\n",
      "Inhaber / Geschäftsführer\n",
      "RESTAURANT_NAME\n",
      "Nachtisch (hausgemachte LOC rote Grütze mit Vanillesauce (winziger Kleks\n",
      "5 halbe, zu harte Pflaumen eine Kugel Movenpick Walnusseis und etwas Sprühsahne\n",
      "Bedienung (Yassin\n",
      "Preis-/Leistungsverhältnis\n",
      "Fusili mit salciche3\n",
      "Fisch & Co\n",
      "Café\n",
      "RESTAURANT_NAME\n",
      "Norma- mit Tomatensauce und Auberginen\n",
      "Location/Ambiente\n",
      "RESTAURANT_NAME\n",
      "Speise-Erlebnis\n",
      "Bedienung, Lena\n",
      "RESTAURANT_NAME\n",
      "Hauptspeise \"Calamares a la plancha\n",
      "Preis-Leistungsverhältnis\n",
      "Curry-Garnelen\n",
      "Preis-Leistungsverhältnis\n",
      "Preis/Leistungsverhältnis\n",
      "LOCes Gose (Bier\n",
      "Sushi-Rollen\n",
      "RESTAURANT_NAME-Team\n",
      "Preis-Leistungsverhältnis\n",
      "Preis/Leistung\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# INPUT_FILE = \"../../11_annotations/experts/tasd_experts_trainset_low.jsonl\"\n",
    "INPUT_FILE = \"../../11_annotations/ground_truth/tasd_testset_low.jsonl\"\n",
    "\n",
    "# regex: only allow letters a-z, A-Z, öäüÖÄÜ\n",
    "valid_phrase_re = re.compile(r\"^[a-zA-ZöäüÖÄÜß ]+$\")\n",
    "\n",
    "specials = []\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        for lbl in entry.get(\"labels\", []):\n",
    "            if len(lbl) >= 3:\n",
    "                phrase = lbl[2]\n",
    "                if phrase != \"NULL\" and not valid_phrase_re.match(phrase):\n",
    "                    specials.append({\n",
    "                        \"id\": entry[\"id\"],\n",
    "                        \"text\": entry[\"text\"],\n",
    "                        \"phrase\": phrase\n",
    "                    })\n",
    "\n",
    "print(f\"Found {len(specials)} special aspect phrases:\")\n",
    "for s in specials[:200]:  # show only first 20 examples\n",
    "    print(s[\"phrase\"])\n",
    "\n",
    "if len(specials) > 200:\n",
    "    print(\"... more not shown\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb95b09",
   "metadata": {},
   "source": [
    "### Transform output to jsonl (Gemma 27B FS -> jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aece42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: z_in\\crowd_acsa_raw.json\n",
      "Processing complete. Cleaned files are in 'results_clean/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Input and output base directories\n",
    "base_dir = 'acsa_experts_llm_fs'\n",
    "output_dir = 'acsa_experts_llm_fs_clean'\n",
    "\n",
    "# Walk through all files in directory\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Load JSON data\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to load {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Remove all keys except selected ones\n",
    "            cleaned_data = []\n",
    "            for obj in data:\n",
    "                cleaned_obj = {\n",
    "                    'id': obj.get('id'),\n",
    "                    'original_id': obj.get('original_id'),\n",
    "                    'text': obj.get('text'),\n",
    "                    'pred_label': obj.get('pred_label')\n",
    "                }\n",
    "                cleaned_data.append(cleaned_obj)\n",
    "\n",
    "            # Construct output path, preserving subfolder structure\n",
    "            rel_path = os.path.relpath(root, base_dir)  # relative to base_dir\n",
    "            target_dir = os.path.join(output_dir, rel_path)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            out_file_path = os.path.join(target_dir, file)\n",
    "\n",
    "            # Save cleaned JSON\n",
    "            with open(out_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Also save as JSONL with \"labels\" format\n",
    "            out_file_path_jsonl = os.path.splitext(out_file_path)[0] + \".jsonl\"\n",
    "            with open(out_file_path_jsonl, 'w', encoding='utf-8') as f_jsonl:\n",
    "                for obj in cleaned_data:\n",
    "                    jsonl_entry = {\n",
    "                        \"id\": obj[\"id\"],\n",
    "                        \"original_id\": obj[\"original_id\"],\n",
    "                        \"text\": obj[\"text\"],\n",
    "                        # wrap pred_label into [[\"category\", \"polarity\", \"term\"]]\n",
    "                        \"labels\": obj.get(\"pred_label\")\n",
    "       \n",
    "                    }\n",
    "                    f_jsonl.write(json.dumps(jsonl_entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Processing complete. Cleaned files are in 'results_clean/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1376653",
   "metadata": {},
   "source": [
    "## Check for duplicates in ACSA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4a7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to: acsa_crowd_llm_ft_clean/acsa_crowd-acsa_test_orig-o_0.0002_16_5_25.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "seed = \"25\"\n",
    "input_file = f\"acsa_crowd_llm_ft/acsa_crowd-acsa_test_orig-o_0.0002_16_5_{seed}.jsonl\"  # original file\n",
    "output_folder = \"acsa_crowd_llm_ft_clean/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file = os.path.join(output_folder, f\"acsa_crowd-acsa_test_orig-o_0.0002_16_5_{seed}.jsonl\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        entry = json.loads(line)\n",
    "        labels = entry.get(\"labels\", [])\n",
    "        \n",
    "        # Remove duplicates by converting list of lists to set of tuples, then back to list\n",
    "        unique_labels = [list(t) for t in set(tuple(lbl) for lbl in labels)]\n",
    "        entry[\"labels\"] = unique_labels\n",
    "        \n",
    "        # Save cleaned entry\n",
    "        json.dump(entry, f_out, ensure_ascii=False)\n",
    "        f_out.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5bb4409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate labels found in id: 648\n",
      "Check complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"acsa_crowd_llm_ft/acsa_crowd-acsa_test_orig-o_0.0002_16_5_25.jsonl\"  # replace with your JSONL file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        labels = entry.get(\"labels\", [])\n",
    "        id = entry.get(\"id\")\n",
    "        \n",
    "        # Check for duplicates by converting list of lists to set of tuples\n",
    "        unique_labels = set(tuple(lbl) for lbl in labels)\n",
    "        if len(unique_labels) < len(labels):\n",
    "            print(f\"Duplicate labels found in id: {entry.get('id')}\")\n",
    "    print(\"Check complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad491c05",
   "metadata": {},
   "source": [
    "### Convert CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "input_file = \"datasets/trainset_1000.csv\"      # your input Excel file\n",
    "output_file = \"datasets/trainset_1000.jsonl\"    # output JSONL file\n",
    "\n",
    "# Read Excel\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as csv_file, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    \n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        jsonl_file.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Converted CSV {input_file} to JSONL {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fc081",
   "metadata": {},
   "source": [
    "### Extract 30 examples from the 50 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd83c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "input_file = \"datasets/50_examples.jsonl\"      # your input file\n",
    "output_file = \"datasets/30_examples.jsonl\"   # file to save selected examples\n",
    "sample_size = 30\n",
    "seed = 14\n",
    "\n",
    "random.seed(seed)\n",
    "# Load JSON\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Check sample size\n",
    "if sample_size > len(data):\n",
    "    raise ValueError(f\"Sample size {sample_size} is greater than dataset size {len(data)}.\")\n",
    "\n",
    "# Random sample\n",
    "sampled_data = random.sample(data, sample_size)\n",
    "\n",
    "# Save back to JSONL\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {sample_size} random examples to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb761096",
   "metadata": {},
   "source": [
    "# Create the original Testset (GERestaurant) into the new format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f3170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Merged file saved as ../../11_annotations/text_only/updatet_labels_original.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "original_file = \"../../11_annotations/text_only/trainset_1000.jsonl\"  # File A\n",
    "file_b = \"../../04_data/datasets/gerestaurant/train.json\"          # File B\n",
    "new_file = \"../../11_annotations/text_only/updatet_labels_original.jsonl\"       # Output file\n",
    "# label mapping\n",
    "label_map = {\n",
    "    \"price\": \"preis\",\n",
    "    \"ambience\": \"ambiente\",\n",
    "    \"service\": \"service\",\n",
    "    \"general impression\": \"gesamteindruck\",\n",
    "    \"food\": \"essen\"\n",
    "}\n",
    "\n",
    "# read File A (jsonl)\n",
    "file_a_records = []\n",
    "with open(original_file, \"r\", encoding=\"utf-8\") as fa:\n",
    "    for line in fa:\n",
    "        file_a_records.append(json.loads(line))\n",
    "\n",
    "# read File B (json or list of jsons)\n",
    "file_b_data = []\n",
    "with open(file_b, \"r\", encoding=\"utf-8\") as fb:\n",
    "    for line in fb:\n",
    "        file_b_data.append(json.loads(line))\n",
    "\n",
    "# create lookup for File B by id\n",
    "file_b_lookup = {str(entry[\"id\"]): entry for entry in file_b_data}\n",
    "\n",
    "# merge\n",
    "merged_records = []\n",
    "for rec in file_a_records:\n",
    "    orig_id = rec.get(\"original_id\")\n",
    "    b_entry = file_b_lookup.get(str(orig_id))\n",
    "    if b_entry:\n",
    "        if rec[\"text\"] == b_entry[\"text\"]:\n",
    "            mapped_labels = []\n",
    "            for label in b_entry.get(\"labels\", []):\n",
    "                mapped_label = label.copy()\n",
    "                if label[0] in label_map:\n",
    "                    mapped_label[0] = label_map[label[0]]\n",
    "                mapped_labels.append(mapped_label)\n",
    "            rec[\"labels\"] = mapped_labels\n",
    "        else:\n",
    "            print(f\"⚠️ Text mismatch for original_id {orig_id}\")\n",
    "    merged_records.append(rec)\n",
    "\n",
    "# write out as jsonl\n",
    "with open(new_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for rec in merged_records:\n",
    "        out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"✅ Done! Merged file saved as {new_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cdde8",
   "metadata": {},
   "source": [
    "### Compare Original and Expert Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42f4d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Comparison Summary ===\n",
      "Same triplets:    1333\n",
      "Changed triplets: 194\n",
      "Added triplets:   92\n",
      "Removed triplets: 102\n",
      "Org: 1435\n",
      "New: 1425\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "file_original = \"../../11_annotations/text_only/updatet_labels_original.jsonl\"  # File A\n",
    "file_experts = \"../../11_annotations/experts/tasd_experts_trainset_low.jsonl\"  # File B\n",
    "\n",
    "# --- Helper: normalize IDs if they start at 1 ---\n",
    "def normalize_id(id_str, offset=0):\n",
    "    \"\"\"Convert string ID to int and apply offset if needed.\"\"\"\n",
    "    return str(int(id_str) - offset)  # subtract offset if needed\n",
    "\n",
    "# --- Read JSONL into dict keyed by normalized ID ---\n",
    "def read_jsonl(file_path, offset=0):\n",
    "    records = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            records[normalize_id(rec[\"id\"], offset=offset)] = rec\n",
    "    return records\n",
    "\n",
    "# Detect ID offset automatically (optional: here we assume file_original is 0-based)\n",
    "original_records = read_jsonl(file_original, offset=0)\n",
    "experts_records  = read_jsonl(file_experts,  offset=1)  # experts IDs start at 1\n",
    "\n",
    "# --- Compare labels ---\n",
    "same_count = 0\n",
    "changed_count = 0\n",
    "added_count = 0\n",
    "removed_count = 0\n",
    "org = 0\n",
    "new = 0\n",
    "diffs = []\n",
    "\n",
    "for id_, rec_orig in original_records.items():\n",
    "    rec_exp = experts_records.get(id_)\n",
    "    if not rec_exp:\n",
    "        continue  # ID missing in experts file\n",
    "\n",
    "    if rec_orig[\"text\"] != rec_exp[\"text\"]:\n",
    "        print(f\"Text mismatch for ID {id_}\")\n",
    "        continue\n",
    "\n",
    "    labels_orig = set(tuple(l) for l in rec_orig.get(\"labels\", []))\n",
    "    \n",
    "    labels_exp  = set(tuple(l) for l in rec_exp.get(\"labels\", []))\n",
    "\n",
    "    same = labels_orig & labels_exp\n",
    "    changed = (labels_orig - labels_exp) | (labels_exp - labels_orig)\n",
    "    s = labels_exp - labels_orig\n",
    "    # print(labels_exp - labels_orig)\n",
    "    same_count += len(same)\n",
    "    added = labels_exp - labels_orig\n",
    "    removed = labels_orig - labels_exp\n",
    "    added_count += len(added)\n",
    "    removed_count += len(removed)\n",
    "    changed_count += len(added) + len(removed)\n",
    "    org += len(labels_orig)\n",
    "    new += len(labels_exp)\n",
    "    # Detailed per-triplet diff\n",
    "    for l in labels_orig - labels_exp:\n",
    "        diffs.append({\"id\": id_, \"text\": rec_orig[\"text\"], \"change\": \"removed\", \"label\": l})\n",
    "    for l in labels_exp - labels_orig:\n",
    "        diffs.append({\"id\": id_, \"text\": rec_orig[\"text\"], \"change\": \"added\", \"label\": l})\n",
    "\n",
    "# --- Summary output ---\n",
    "print(\"=== Label Comparison Summary ===\")\n",
    "print(f\"Same triplets:    {same_count}\")\n",
    "print(f\"Changed triplets: {changed_count}\")\n",
    "print(f\"Added triplets:   {added_count}\")\n",
    "print(f\"Removed triplets: {removed_count}\")\n",
    "print(f\"Org: {org}\")\n",
    "print(f\"New: {new}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40446c5d",
   "metadata": {},
   "source": [
    "### Transform single GT Annotation from TASD to ACSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0786d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# input and output folder paths\n",
    "input_folder = \"../../11_annotations/ground_truth/Single_Annotations_TASD\"\n",
    "output_folder = \"../../11_annotations/ground_truth/Single_Annotations_ACSA\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# polarity mapping\n",
    "polarity_map = {\n",
    "    \"positiv\": \"positiv\",\n",
    "    \"negativ\": \"negativ\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"conflict\": \"konflikt\"\n",
    "}\n",
    "\n",
    "# process each file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "             open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                data = json.loads(line)\n",
    "\n",
    "                new_labels = []\n",
    "                for label in data.get(\"labels\", []):\n",
    "                    if len(label) >= 2:\n",
    "                        category = label[0].lower()\n",
    "                        polarity = label[1].lower()\n",
    "                        polarity = polarity_map.get(polarity, polarity)\n",
    "                        new_labels.append([category, polarity])\n",
    "\n",
    "                data[\"labels\"] = new_labels\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f083a2",
   "metadata": {},
   "source": [
    "## Remove Conflict and Duplicates for ACSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13208a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# input and output folder paths\n",
    "input_folder = \"../../11_annotations/ground_truth/Single_Annotations_ACSA\"\n",
    "output_folder = \"../../11_annotations/ground_truth/Single_Annotations_ACSA_processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# polarity mapping\n",
    "polarity_map = {\n",
    "    \"positiv\": \"positiv\",\n",
    "    \"negativ\": \"negativ\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"conflict\": \"konflikt\"\n",
    "}\n",
    "\n",
    "# process each file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".json\"):  # treat .json as jsonl\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename.replace(\".json\", \".jsonl\"))\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "             open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                data = json.loads(line)\n",
    "\n",
    "                new_labels = []\n",
    "                for label in data.get(\"labels\", []):\n",
    "                    if len(label) >= 2:\n",
    "                        category = label[0].lower()\n",
    "                        polarity = label[1].lower()\n",
    "                        polarity = polarity_map.get(polarity, polarity)\n",
    "\n",
    "                        # skip konflikt\n",
    "                        if polarity == \"konflikt\":\n",
    "                            continue\n",
    "\n",
    "                        new_labels.append((category, polarity))  # use tuple for deduplication\n",
    "\n",
    "                # remove duplicates while preserving order\n",
    "                seen = set()\n",
    "                unique_labels = []\n",
    "                for lbl in new_labels:\n",
    "                    if lbl not in seen:\n",
    "                        seen.add(lbl)\n",
    "                        unique_labels.append(list(lbl))  # convert tuple back to list\n",
    "\n",
    "                data[\"labels\"] = unique_labels\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1df2d",
   "metadata": {},
   "source": [
    "### Replace ids and transform to string (ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# input and output folder paths\n",
    "input_folder = \"../../11_annotations/text_only/updatet_labels_original.jsonl\"\n",
    "output_folder = \"../../11_annotations/text_only/updatet_labels_original.jsonl\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# regex to extract start/end ids from filename\n",
    "pattern = re.compile(r\".*_(\\d+)_(\\d+)_.*\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        match = pattern.match(filename)\n",
    "        if not match:\n",
    "            print(f\"⚠️ Skipping {filename}, cannot extract id range\")\n",
    "            continue\n",
    "\n",
    "        start_id, end_id = map(int, match.groups())\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            lines = infile.readlines()\n",
    "\n",
    "        if len(lines) != (end_id - start_id + 1):\n",
    "            print(f\"⚠️ Warning: {filename} has {len(lines)} lines but expected {end_id - start_id + 1}\")\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for idx, line in enumerate(lines, start=start_id):\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # assign new string id\n",
    "                data[\"id\"] = str(idx)\n",
    "\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
