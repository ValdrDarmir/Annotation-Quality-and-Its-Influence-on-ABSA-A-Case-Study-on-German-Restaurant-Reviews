{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c209713-c281-47f8-8d3e-5da05b0e88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "!source ../../../scripts/mvp/bin/activate\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "%set_env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8cc5f0e-93dc-45cc-84c0-54905c13ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from itertools import permutations\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "utils = os.path.abspath('../../utils/') # Relative path to utils scripts\n",
    "sys.path.append(utils)\n",
    "\n",
    "from evaluation import createResults, convertLabels\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar, LearningRateMonitor\n",
    "\n",
    "from transformers import AdamW, T5Tokenizer, T5ForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.models.t5.modeling_t5 import *\n",
    "\n",
    "from const import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40c45b29-8bd4-499d-a966-49c011e9e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 5\n"
     ]
    }
   ],
   "source": [
    "LABEL_SPACE = ['ambience general:POSITIVE', 'ambience general:NEUTRAL', 'ambience general:NEGATIVE', 'drinks prices:POSITIVE', 'drinks prices:NEUTRAL', 'drinks prices:NEGATIVE', 'drinks quality:POSITIVE', 'drinks quality:NEUTRAL', 'drinks quality:NEGATIVE', 'drinks style_options:POSITIVE', 'drinks style_options:NEUTRAL', 'drinks style_options:NEGATIVE', 'food prices:POSITIVE', 'food prices:NEUTRAL', 'food prices:NEGATIVE', 'food quality:POSITIVE', 'food quality:NEUTRAL', 'food quality:NEGATIVE', 'food style_options:POSITIVE', 'food style_options:NEUTRAL', 'food style_options:NEGATIVE', 'location general:POSITIVE', 'location general:NEUTRAL', 'location general:NEGATIVE', 'restaurant general:POSITIVE', 'restaurant general:NEUTRAL', 'restaurant general:NEGATIVE', 'restaurant miscellaneous:POSITIVE', 'restaurant miscellaneous:NEUTRAL', 'restaurant miscellaneous:NEGATIVE', 'restaurant prices:POSITIVE', 'restaurant prices:NEUTRAL', 'restaurant prices:NEGATIVE', 'service general:POSITIVE', 'service general:NEUTRAL', 'service general:NEGATIVE']\n",
    "\n",
    "LANGUAGES = ['nl', 'de', 'en', 'cs', 'ru', 'fr', 'es']\n",
    "\n",
    "\n",
    "def extract_spans_para(seq, seq_type):\n",
    "    quads = []\n",
    "    sents = [s.strip() for s in seq.split('[SSEP]')]\n",
    "    for s in sents:\n",
    "        try:\n",
    "            tok_list = [\"[C]\", \"[S]\", \"[A]\", \"[O]\"]\n",
    "\n",
    "            for tok in tok_list:\n",
    "                if tok not in s:\n",
    "                    s += \" {} null\".format(tok)\n",
    "            index_ac = s.index(\"[C]\")\n",
    "            index_sp = s.index(\"[S]\")\n",
    "            index_at = s.index(\"[A]\")\n",
    "            index_ot = s.index(\"[O]\")\n",
    "\n",
    "            combined_list = [index_ac, index_sp, index_at, index_ot]\n",
    "            arg_index_list = list(np.argsort(combined_list))\n",
    "\n",
    "            result = []\n",
    "            for i in range(len(combined_list)):\n",
    "                start = combined_list[i] + 4\n",
    "                sort_index = arg_index_list.index(i)\n",
    "                if sort_index < 3:\n",
    "                    next_ = arg_index_list[sort_index + 1]\n",
    "                    re = s[start:combined_list[next_]]\n",
    "                else:\n",
    "                    re = s[start:]\n",
    "                result.append(re.strip())\n",
    "\n",
    "            ac, sp, at, ot = result\n",
    "\n",
    "            # if the aspect term is implicit\n",
    "            if at.lower() == 'it' or at.lower() == 'es':\n",
    "                at = 'null'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                print(f'In {seq_type} seq, cannot decode: {s}')\n",
    "                pass\n",
    "            except UnicodeEncodeError:\n",
    "                print(f'In {seq_type} seq, a string cannot be decoded')\n",
    "                pass\n",
    "            ac, at, sp, ot = '', '', '', ''\n",
    "\n",
    "        quads.append((ac, at, sp, ot))\n",
    "\n",
    "    return quads\n",
    "\n",
    "\n",
    "def compute_f1_scores(pred_pt, gold_pt, verbose=False):\n",
    "    \"\"\"\n",
    "    Function to compute F1 scores with pred and gold quads\n",
    "    The input needs to be already processed\n",
    "    \"\"\"\n",
    "    # number of true postive, gold standard, predictions\n",
    "    n_tp, n_gold, n_pred = 0, 0, 0\n",
    "\n",
    "    for i in range(len(pred_pt)):\n",
    "        n_gold += len(gold_pt[i])\n",
    "        n_pred += len(pred_pt[i])\n",
    "\n",
    "        for t in pred_pt[i]:\n",
    "            if t in gold_pt[i]:\n",
    "                n_tp += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"number of gold spans: {n_gold}, predicted spans: {n_pred}, hit: {n_tp}\"\n",
    "        )\n",
    "\n",
    "    precision = float(n_tp) / float(n_pred) if n_pred != 0 else 0\n",
    "    recall = float(n_tp) / float(n_gold) if n_gold != 0 else 0\n",
    "    f1 = 2 * precision * recall / (\n",
    "        precision + recall) if precision != 0 or recall != 0 else 0\n",
    "    scores = {\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1': f1 * 100\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_scores(pred_seqs, gold_seqs, verbose=True, task=\"asqp\"):\n",
    "    \"\"\"\n",
    "    Compute model performance\n",
    "    \"\"\"\n",
    "    assert len(pred_seqs) == len(gold_seqs), (len(pred_seqs), len(gold_seqs))\n",
    "    num_samples = len(gold_seqs)\n",
    "\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        gold_list = extract_spans_para(gold_seqs[i], 'gold')\n",
    "        pred_list = extract_spans_para(pred_seqs[i], 'pred')\n",
    "        if (task == \"tasd\"):\n",
    "          gold_list = [tup[:-1] for tup in gold_list]\n",
    "          pred_list = [tup[:-1] for tup in pred_list]\n",
    "\n",
    "        all_labels.append(gold_list)\n",
    "        all_preds.append(pred_list)\n",
    "\n",
    "    try:\n",
    "        preds = [[f'{labels[0]}:{opinion2sentword[labels[2]].upper() if labels[2] in opinion2sentword else \"\"}:{labels[1]}' for labels in pred] for pred in all_preds]\n",
    "        golds = [[f'{labels[0]}:{opinion2sentword[labels[2]].upper() if labels[2] in opinion2sentword else \"\"}:{labels[1]}' for labels in gold] for gold in all_labels]\n",
    "    except KeyError:\n",
    "        print('KeyError!')\n",
    "        print(all_labels)\n",
    "    except:\n",
    "        print(all_labels)\n",
    "\n",
    "    scores_dfs = createResults(preds, golds, LABEL_SPACE, task)\n",
    "    \n",
    "    scores = compute_f1_scores(all_preds, all_labels)\n",
    "    scores[\"all_preds\"] = all_preds\n",
    "    scores[\"all_labels\"] = all_labels\n",
    "    print('MVP F1-Micro: ', scores['f1'])\n",
    "    # return scores, all_labels, all_preds\n",
    "    return scores_dfs, all_labels, pred_seqs\n",
    "\n",
    "\n",
    "def get_element_tokens(task):\n",
    "    dic = {\n",
    "        \"aste\":\n",
    "            [\"[A]\", \"[O]\", \"[S]\"],\n",
    "        \"tasd\":\n",
    "            [\"[A]\", \"[C]\", \"[S]\"],\n",
    "        \"aocs\":\n",
    "        [\"[A]\", \"[O]\", \"[C]\", \"[S]\"],\n",
    "        \"asqp\":\n",
    "            [\"[A]\", \"[O]\", \"[C]\", \"[S]\"],\n",
    "    }\n",
    "    return dic[task]\n",
    "\n",
    "optim_orders_all = None\n",
    "\n",
    "def get_orders(task, data, data_type, args, sents, labels):\n",
    "    # Empfehlung: Optimale Reihenfolge der Elemente vorab 1x berechnen. \n",
    "    # uncomment to calculate orders from scratch\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "          device = torch.device('cuda')\n",
    "    else:\n",
    "          device = torch.device(\"cpu\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)\n",
    "    model = MyT5ForConditionalGenerationScore.from_pretrained(\n",
    "            args.model_name_or_path).to(device)\n",
    "\n",
    "    # if args.dataset in optim_orders_all_predefined[task].keys():\n",
    "    #     if args.\n",
    "    \n",
    "    global optim_orders_all\n",
    "\n",
    "    if optim_orders_all == None:\n",
    "        optim_orders_all = choose_best_order_global(sents, labels, model,\n",
    "                                             tokenizer, device,\n",
    "                                             args.task)\n",
    "\n",
    "    #     print(optim_orders_all)\n",
    "\n",
    "    if args.single_view_type == 'rank':\n",
    "           orders = optim_orders_all#[task][\"rest16\"] # delete [task][data] falls selber berechnet werden\n",
    "    elif args.single_view_type == 'rand':\n",
    "           orders = [random.Random(args.seed).choice(\n",
    "               optim_orders_all[task][data])]\n",
    "    elif args.single_view_type == \"heuristic\":\n",
    "           orders = heuristic_orders[task]\n",
    "\n",
    "    del model\n",
    "    return orders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cal_entropy(inputs, preds, model_path, tokenizer, device=torch.device('cuda')):\n",
    "    all_entropy = []\n",
    "    model = MyT5ForConditionalGenerationScore.from_pretrained(model_path).to(\n",
    "        device)\n",
    "    batch_size = 8\n",
    "    _inputs = [' '.join(s) for s in inputs]\n",
    "    _preds = [' '.join(s) for s in preds]\n",
    "    for id in range(0, len(inputs), batch_size):\n",
    "        in_batch = _inputs[id: min(id + batch_size, len(inputs))]\n",
    "        pred_batch = _preds[id: min(id + batch_size, len(inputs))]\n",
    "        assert len(in_batch) == len(pred_batch)\n",
    "        tokenized_input = tokenizer.batch_encode_plus(in_batch,\n",
    "                                                      max_length=200,\n",
    "                                                      padding=\"max_length\",\n",
    "                                                      truncation=True,\n",
    "                                                      return_tensors=\"pt\")\n",
    "        tokenized_target = tokenizer.batch_encode_plus(pred_batch,\n",
    "                                                       max_length=200,\n",
    "                                                       padding=\"max_length\",\n",
    "                                                       truncation=True,\n",
    "                                                       return_tensors=\"pt\")\n",
    "\n",
    "        target_ids = tokenized_target[\"input_ids\"].to(device)\n",
    "\n",
    "        target_ids[target_ids[:, :] == tokenizer.pad_token_id] = -100\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_input[\"input_ids\"].to(device),\n",
    "            attention_mask=tokenized_input[\"attention_mask\"].to(device),\n",
    "            labels=target_ids,\n",
    "            decoder_attention_mask=tokenized_target[\"attention_mask\"].to(device))\n",
    "\n",
    "        loss, entropy = outputs[0]\n",
    "        all_entropy.extend(entropy)\n",
    "    return all_entropy\n",
    "\n",
    "\n",
    "def order_scores_function(quad_list, cur_sent, model, tokenizer, device, task):\n",
    "    q = get_element_tokens(task)\n",
    "\n",
    "    all_orders = permutations(q)\n",
    "    all_orders_list = []\n",
    "\n",
    "    all_targets = []\n",
    "    all_inputs = []\n",
    "    cur_sent = \" \".join(cur_sent)\n",
    "\n",
    "    for each_order in all_orders:\n",
    "        cur_order = \"  \".join(each_order) + \" \"\n",
    "        all_orders_list.append(cur_order)\n",
    "        cur_target = []\n",
    "        for each_q in quad_list:\n",
    "            cur_target.append(each_q[cur_order][0])\n",
    "\n",
    "        all_inputs.append(cur_sent)\n",
    "        all_targets.append(\" \".join(cur_target))\n",
    "\n",
    "    tokenized_input = tokenizer.batch_encode_plus(all_inputs,\n",
    "                                                  max_length=200,\n",
    "                                                  padding=\"max_length\",\n",
    "                                                  truncation=True,\n",
    "                                                  return_tensors=\"pt\")\n",
    "    tokenized_target = tokenizer.batch_encode_plus(all_targets,\n",
    "                                                   max_length=200,\n",
    "                                                   padding=\"max_length\",\n",
    "                                                   truncation=True,\n",
    "                                                   return_tensors=\"pt\")\n",
    "\n",
    "    target_ids = tokenized_target[\"input_ids\"].to(device)\n",
    "\n",
    "    target_ids[target_ids[:, :] == tokenizer.pad_token_id] = -100\n",
    "    outputs = model(\n",
    "        input_ids=tokenized_input[\"input_ids\"].to(device),\n",
    "        attention_mask=tokenized_input[\"attention_mask\"].to(device),\n",
    "        labels=target_ids,\n",
    "        decoder_attention_mask=tokenized_target[\"attention_mask\"].to(device))\n",
    "\n",
    "    loss, entropy = outputs[0]\n",
    "    results = {}\n",
    "    for i, _ in enumerate(all_orders_list):\n",
    "        cur_order = all_orders_list[i]\n",
    "        results[cur_order] = {\"loss\": loss[i], \"entropy\": entropy[i]}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def choose_best_order_global(sents, labels, model, tokenizer, device, task):\n",
    "    q = get_element_tokens(task)\n",
    "    all_orders = permutations(q)\n",
    "    all_orders_list = []\n",
    "    scores = []\n",
    "\n",
    "    for each_order in all_orders:\n",
    "        cur_order = \" \".join(each_order)\n",
    "        all_orders_list.append(cur_order)\n",
    "        scores.append(0)\n",
    "\n",
    "    for i in range(len(sents)):\n",
    "        label = labels[i]\n",
    "        sent = sents[i]\n",
    "\n",
    "        quad_list = []\n",
    "        for _tuple in label:\n",
    "            # parse ASTE tuple\n",
    "            if task == \"aste\":\n",
    "                _tuple = parse_aste_tuple(_tuple, sent)\n",
    "\n",
    "            at, ac, sp, ot = get_task_tuple(_tuple, task, args.lang)\n",
    "\n",
    "            element_dict = {\"[A]\": at, \"[O]\": ot, \"[C]\": ac, \"[S]\": sp}\n",
    "            element_list = []\n",
    "            for key in q:\n",
    "                element_list.append(\"{} {}\".format(key, element_dict[key]))\n",
    "\n",
    "            x = permutations(element_list)\n",
    "\n",
    "            permute_object = {}\n",
    "            for each in x:\n",
    "                order = []\n",
    "                content = []\n",
    "                for e in each:\n",
    "                    order.append(e[0:4])\n",
    "                    content.append(e[4:])\n",
    "                order_name = \" \".join(order)\n",
    "                content = \" \".join(content)\n",
    "                permute_object[order_name] = [content, \" \".join(each)]\n",
    "\n",
    "            quad_list.append(permute_object)\n",
    "\n",
    "        order_scores = order_scores_function(quad_list, sent, model, tokenizer,\n",
    "                                             device, task)\n",
    "\n",
    "        for e in order_scores:\n",
    "            e = e[:-1].replace(\"  \", \" \")\n",
    "            index = all_orders_list.index(e)\n",
    "            scores[index] += order_scores[e.replace(\" \", \"  \")+\" \"]['entropy']\n",
    "\n",
    "    indexes = np.argsort(np.array(scores))  # [::-1]\n",
    "    returned_orders = []\n",
    "    for i in indexes:\n",
    "        returned_orders.append(all_orders_list[i])\n",
    "\n",
    "    return returned_orders\n",
    "\n",
    "\n",
    "def parse_aste_tuple(_tuple, sent):\n",
    "    if isinstance(_tuple[0], str):\n",
    "        res = _tuple\n",
    "    elif isinstance(_tuple[0], list):\n",
    "        # parse at\n",
    "        start_idx = _tuple[0][0]\n",
    "        end_idx = _tuple[0][-1] if len(_tuple[0]) > 1 else start_idx\n",
    "        at = ' '.join(sent[start_idx:end_idx + 1])\n",
    "\n",
    "        # parse ot\n",
    "        start_idx = _tuple[1][0]\n",
    "        end_idx = _tuple[1][-1] if len(_tuple[1]) > 1 else start_idx\n",
    "        ot = ' '.join(sent[start_idx:end_idx + 1])\n",
    "        res = [at, ot, _tuple[2]]\n",
    "    else:\n",
    "        print(_tuple)\n",
    "        raise NotImplementedError\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_task_tuple(_tuple, task, lang):\n",
    "    if task == \"aste\":\n",
    "        at, ot, sp = _tuple\n",
    "        ac = None\n",
    "    elif task == \"tasd\":\n",
    "        at, ac, sp = _tuple\n",
    "        ot = None\n",
    "    elif task in [\"asqp\", \"acos\"]:\n",
    "        at, ac, sp, ot = _tuple\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if sp:\n",
    "        if True:\n",
    "        # if lang == 'en':\n",
    "            sp = sentword2opinion[sp.lower()] if sp in sentword2opinion \\\n",
    "                else senttag2opinion[sp.lower()]  # 'POS' -> 'good'\n",
    "        # else:\n",
    "        #     sp = sentword2opinion_GER[sp.lower()] if sp in sentword2opinion \\\n",
    "        #         else senttag2opinion_GER[sp.lower()]  # 'POS' -> 'gut'\n",
    "    if at and at.lower() == 'null':  # for implicit aspect term\n",
    "        if True:\n",
    "        # if lang == 'en':\n",
    "            at = 'it'\n",
    "        # else:\n",
    "        #     at = 'es'\n",
    "\n",
    "    return at, ac, sp, ot\n",
    "\n",
    "\n",
    "def add_prompt(sent, orders, task, data_name, args):\n",
    "\n",
    "    # add ctrl_token\n",
    "    if args.ctrl_token == \"none\":\n",
    "        pass\n",
    "    elif args.ctrl_token == \"post\":\n",
    "        sent = sent + orders\n",
    "    elif args.ctrl_token == \"pre\":\n",
    "        sent = orders + sent\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return sent\n",
    "\n",
    "\n",
    "def get_para_targets(sents, labels, data_name, data_type, top_k, task, args):\n",
    "    \"\"\"\n",
    "    Obtain the target sentence under the paraphrase paradigm\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    new_sents = []\n",
    "    if task in ['aste', 'tasd']:\n",
    "        # at most 5 orders for triple tasks\n",
    "        top_k = min(5, top_k)\n",
    "\n",
    "    optim_orders = get_orders(task, data_name, data_type, args, sents, labels)[:top_k]\n",
    "\n",
    "    for i in range(len(sents)):\n",
    "        label = labels[i]\n",
    "        cur_sent = sents[i]\n",
    "        cur_sent_str = \" \".join(cur_sent)\n",
    "\n",
    "        # ASTE: parse at & ot\n",
    "        if task == 'aste':\n",
    "            assert len(label[0]) == 3\n",
    "            parsed_label = []\n",
    "            for _tuple in label:\n",
    "                parsed_tuple = parse_aste_tuple(_tuple, sents[i])\n",
    "                parsed_label.append(parsed_tuple)\n",
    "            label = parsed_label\n",
    "\n",
    "        # sort label by order of appearance\n",
    "        # at, ac, sp, ot\n",
    "        if args.sort_label and len(label) > 1:\n",
    "            label_pos = {}\n",
    "            for _tuple in label:\n",
    "                at, ac, sp, ot = get_task_tuple(_tuple, task, args.lang)\n",
    "\n",
    "                # get last at / ot position\n",
    "                at_pos = cur_sent_str.find(at) if at else -1\n",
    "                ot_pos = cur_sent_str.find(ot) if ot else -1\n",
    "                last_pos = max(at_pos, ot_pos)\n",
    "                last_pos = 1e4 if last_pos < 0 else last_pos\n",
    "                label_pos[tuple(_tuple)] = last_pos\n",
    "            new_label = [\n",
    "                list(k)\n",
    "                for k, _ in sorted(label_pos.items(), key=lambda x: x[1])\n",
    "            ]\n",
    "            label = new_label\n",
    "\n",
    "        quad_list = []\n",
    "        for _tuple in label:\n",
    "            at, ac, sp, ot = get_task_tuple(_tuple, task, args.lang)\n",
    "            element_dict = {\"[A]\": at, \"[O]\": ot, \"[C]\": ac, \"[S]\": sp}\n",
    "            token_end = 3\n",
    "\n",
    "            element_list = []\n",
    "            for key in optim_orders[0].split(\" \"):\n",
    "                element_list.append(\"{} {}\".format(key, element_dict[key]))\n",
    "\n",
    "            x = permutations(element_list)\n",
    "            permute_object = {}\n",
    "            for each in x:\n",
    "                order = []\n",
    "                content = []\n",
    "                for e in each:\n",
    "                    order.append(e[0:token_end])\n",
    "                    content.append(e[token_end:])\n",
    "                order_name = \" \".join(order)\n",
    "                content = \" \".join(content)\n",
    "                permute_object[order_name] = [content, \" \".join(each)]\n",
    "\n",
    "            quad_list.append(permute_object)\n",
    "\n",
    "        for o in optim_orders:\n",
    "            tar = []\n",
    "            for each_q in quad_list:\n",
    "                tar.append(each_q[o][1])\n",
    "\n",
    "            targets.append(\" [SSEP] \".join(tar))\n",
    "            # add prompt\n",
    "            new_sent = add_prompt(cur_sent, o.split(), task, data_name, args)\n",
    "            new_sents.append(new_sent)\n",
    "\n",
    "    return new_sents, targets\n",
    "\n",
    "\n",
    "def get_para_targets_dev(sents, labels, data_name, task, args):\n",
    "    \"\"\"\n",
    "    Obtain the target sentence under the paraphrase paradigm\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    targets = []\n",
    "    optim_orders = get_orders(task, data_name, \"test\", args, sents=None, labels=None)\n",
    "    top_order = optim_orders[0].split(\" \")\n",
    "    for sent, label in zip(sents, labels):\n",
    "        all_quad_sentences = []\n",
    "        for _tuple in label:\n",
    "            # parse ASTE tuple\n",
    "            if task == \"aste\":\n",
    "                _tuple = parse_aste_tuple(_tuple, sent)\n",
    "\n",
    "            at, ac, sp, ot = get_task_tuple(_tuple, task, args.lang)\n",
    "\n",
    "            element_dict = {\"[A]\": at, \"[O]\": ot, \"[C]\": ac, \"[S]\": sp}\n",
    "            element_list = []\n",
    "            for key in top_order:\n",
    "                element_list.append(\"{} {}\".format(key, element_dict[key]))\n",
    "\n",
    "            one_quad_sentence = \" \".join(element_list)\n",
    "            all_quad_sentences.append(one_quad_sentence)\n",
    "\n",
    "        target = ' [SSEP] '.join(all_quad_sentences)\n",
    "        targets.append(target)\n",
    "\n",
    "        # add prompt\n",
    "        sent = add_prompt(sent, top_order, task, data_name, args)\n",
    "\n",
    "        new_sents.append(sent)\n",
    "    return new_sents, targets\n",
    "    \n",
    "def formatText(text):\n",
    "    text = re.sub(r'([(\".,!?;:/)])', r\" \\1\", text)\n",
    "    text = re.sub(r'([\"„“…])', r'', text)\n",
    "    text = re.sub(r'([\\'])', r' \\1', text)\n",
    "    # text = re.sub(r'([-])', r' \\1 ', text)\n",
    "    text = re.sub(r'([\\s\\s])', r' ', text)\n",
    "    text = re.sub(r\"\\b(I|You|We|They|He|She|It|Don|Didn|Doesn|Can|Couldn|Wouldn|Shouldn|Won|Would|Wasn|Aren|Ain|Isn|Hasn|Haven|Weren|Mightn|Mustn)('|’)(m|t|ll|ve|re|s|d)\\b\", r\"\\1 \\2\\3\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "def read_line_examples_from_file(data_path,\n",
    "                                 task_name,\n",
    "                                 data_name,\n",
    "                                 lowercase,\n",
    "                                 silence=True):\n",
    "    \"\"\"\n",
    "    Read data from file, each line is: sent####labels\n",
    "    Return List[List[word]], List[Tuple]\n",
    "    \"\"\"\n",
    "    tasks, datas = [], []\n",
    "    sents, labels = [], []\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        df = pd.read_json(f, orient=\"records\", lines=True).set_index('id')\n",
    "        for index, row in df.iterrows():\n",
    "            tasks.append(task_name)\n",
    "            datas.append(data_name)\n",
    "            if lowercase:\n",
    "                sents.append(formatText(row['text'].lower()).split())\n",
    "                labels.append([(label[2].lower(), label[0], label[1]) for label in row['labels']])\n",
    "            else:\n",
    "                sents.append(formatText(row['text']).split())\n",
    "                labels.append([(label[2], label[0], label[1]) for label in row['labels']])\n",
    "\n",
    "    if silence:\n",
    "        print(f\"Total examples = {len(sents)}\")\n",
    "    return tasks, datas, sents, labels\n",
    "\n",
    "def get_transformed_io(data_path, data_name, data_type, top_k, args):\n",
    "    \"\"\"\n",
    "    The main function to transform input & target according to the task\n",
    "    \"\"\"\n",
    "    tasks, datas, sents, labels = read_line_examples_from_file(\n",
    "        data_path, args.task, args.dataset, args.lowercase)\n",
    "\n",
    "    # the input is just the raw sentence\n",
    "    inputs = [s.copy() for s in sents]\n",
    "\n",
    "    # low resource\n",
    "    if data_type == 'train' and args.data_ratio != 1.0:\n",
    "        num_sample = int(len(inputs) * args.data_ratio)\n",
    "        sample_indices = random.sample(list(range(0, len(inputs))), num_sample)\n",
    "        sample_inputs = [inputs[i] for i in sample_indices]\n",
    "        sample_labels = [labels[i] for i in sample_indices]\n",
    "        inputs, labels = sample_inputs, sample_labels\n",
    "        print(\n",
    "            f\"Low resource: {args.data_ratio}, total train examples = {num_sample}\")\n",
    "        if num_sample <= 20:\n",
    "            print(\"Labels:\", sample_labels)\n",
    "\n",
    "    if data_type == \"train\" or args.eval_type == \"dev\" or data_type == \"test\":\n",
    "        new_inputs, targets = get_para_targets(inputs, labels, data_name,\n",
    "                                               data_type, top_k, args.task,\n",
    "                                               args)\n",
    "    else:\n",
    "        new_inputs, targets = get_para_targets_dev(inputs, labels, data_name,\n",
    "                                                   args.task, args)\n",
    "\n",
    "    print(len(inputs), len(new_inputs), len(targets))\n",
    "    return new_inputs, targets\n",
    "\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 task_name,\n",
    "                 data_setting,\n",
    "                 data_name,\n",
    "                 language,\n",
    "                 data_type,\n",
    "                 top_k,\n",
    "                 args,\n",
    "                 max_len=128):\n",
    "\n",
    "        \n",
    "        if data_type == 'train':\n",
    "            print('Loading Train Dataset')\n",
    "            if 'multi' in data_setting:\n",
    "                self.data_path = []\n",
    "                for lang in [lang for lang in LANGUAGES if lang != language]:\n",
    "                    self.data_path.append(f'{args.data_path}/{lang}/train_b.json')\n",
    "                if data_setting == 'multi_id':\n",
    "                    self.data_path.append(f'{args.data_path}/{language}/train_b.json')\n",
    "            else:\n",
    "                self.data_path = f'{args.data_path}/{language}/train{\"_b\" if data_setting == \"balanced\" else \"\"}.json'\n",
    "        elif data_type == 'val':\n",
    "            print('Loading Validation Dataset')\n",
    "            self.data_path = f'{args.data_path}/{language}/test{\"_b\" if data_setting == \"balanced\" else \"\"}.json'\n",
    "        else:\n",
    "            print('Loading Test Dataset')\n",
    "            self.data_path = f'{args.data_path}/{language}/test{\"_b\" if data_setting == \"balanced\" else \"\"}.json'\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_name = task_name\n",
    "        self.language = language\n",
    "        self.data_setting = data_setting\n",
    "        self.data_name = data_name\n",
    "        self.data_type = data_type\n",
    "        self.args = args\n",
    "\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build_examples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze(\n",
    "        )  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze(\n",
    "        )  # might need to squeeze\n",
    "        return {\n",
    "            \"source_ids\": source_ids,\n",
    "            \"source_mask\": src_mask,\n",
    "            \"target_ids\": target_ids,\n",
    "            \"target_mask\": target_mask\n",
    "        }\n",
    "\n",
    "    def _build_examples(self):\n",
    "\n",
    "\n",
    "        if type(self.data_path) == list:\n",
    "            for data_path in self.data_path:\n",
    "                if self.args.multi_task:\n",
    "                    inputs, targets = get_transformed_io_unified(\n",
    "                        data_path, self.task_name, f'{self.data_name}-{self.language}', self.data_type,\n",
    "                        self.top_k, self.args)\n",
    "                else:\n",
    "                    inputs, targets = get_transformed_io(data_path,\n",
    "                                                         f'{self.data_name}-{self.language}',\n",
    "                                                         self.data_type, self.top_k,\n",
    "                                                         self.args)\n",
    "        \n",
    "                for i in range(len(inputs)):\n",
    "                    # change input and target to two strings\n",
    "                    input = ' '.join(inputs[i])\n",
    "                    target = targets[i]\n",
    "        \n",
    "                    tokenized_input = self.tokenizer.batch_encode_plus(\n",
    "                        [input],\n",
    "                        max_length=self.max_len,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\")\n",
    "                    \n",
    "                    # for ACOS Restaurant and Laptop dataset\n",
    "                    # the max target length is much longer than 200\n",
    "                    # we need to set a larger max length for inference\n",
    "                    target_max_length = 1024 if self.data_type == \"test\" else self.max_len\n",
    "        \n",
    "                    tokenized_target = self.tokenizer.batch_encode_plus(\n",
    "                        [target],\n",
    "                        max_length=target_max_length,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\")\n",
    "        \n",
    "                    self.inputs.append(tokenized_input)\n",
    "                    self.targets.append(tokenized_target)\n",
    "        else:\n",
    "            if self.args.multi_task:\n",
    "                inputs, targets = get_transformed_io_unified(\n",
    "                    self.data_path, self.task_name, f'{self.data_name}-{self.language}', self.data_type,\n",
    "                    self.top_k, self.args)\n",
    "            else:\n",
    "                inputs, targets = get_transformed_io(self.data_path,\n",
    "                                                     f'{self.data_name}-{self.language}',\n",
    "                                                     self.data_type, self.top_k,\n",
    "                                                     self.args)\n",
    "    \n",
    "            for i in range(len(inputs)):\n",
    "                # change input and target to two strings\n",
    "                input = ' '.join(inputs[i])\n",
    "                target = targets[i]\n",
    "    \n",
    "                tokenized_input = self.tokenizer.batch_encode_plus(\n",
    "                    [input],\n",
    "                    max_length=self.max_len,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\")\n",
    "                \n",
    "                # for ACOS Restaurant and Laptop dataset\n",
    "                # the max target length is much longer than 200\n",
    "                # we need to set a larger max length for inference\n",
    "                target_max_length = 1024 if self.data_type == \"test\" else self.max_len\n",
    "    \n",
    "                tokenized_target = self.tokenizer.batch_encode_plus(\n",
    "                    [target],\n",
    "                    max_length=target_max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\")\n",
    "    \n",
    "                self.inputs.append(tokenized_input)\n",
    "                self.targets.append(tokenized_target)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"T5Config\"\n",
    "\n",
    "def calc_entropy(input_tensor):\n",
    "    lsm = nn.LogSoftmax()\n",
    "    log_probs = lsm(input_tensor)\n",
    "    probs = torch.exp(log_probs)\n",
    "    p_log_p = log_probs * probs\n",
    "    entropy = -p_log_p.sum()\n",
    "    return entropy\n",
    "\n",
    "# add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING)\n",
    "@add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING)\n",
    "class MyT5ForConditionalGenerationScore(T5PreTrainedModel):\n",
    "    authorized_missing_keys = [r\"encoder\\.embed_tokens\\.weight\", r\"decoder\\.embed_tokens\\.weight\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        encoder_config.use_cache = False\n",
    "        encoder_config.is_encoder_decoder = False\n",
    "        self.encoder = T5Stack(encoder_config, self.shared)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.is_encoder_decoder = False\n",
    "        decoder_config.num_layers = config.num_decoder_layers\n",
    "        self.decoder = T5Stack(decoder_config, self.shared)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "        self.encoder.set_input_embeddings(new_embeddings)\n",
    "        self.decoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to ``-100`` are ignored (masked), the loss is only computed for\n",
    "            labels in ``[0, ..., config.vocab_size]``\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\n",
    "\n",
    "            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='pt').input_ids\n",
    "            >>> labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='pt').input_ids\n",
    "            >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "            >>> loss = outputs.loss\n",
    "            >>> logits = outputs.logits\n",
    "\n",
    "            >>> input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you \", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "            >>> outputs = model.generate(input_ids)\n",
    "        \"\"\"\n",
    "\n",
    "        if \"lm_labels\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"lm_labels\")\n",
    "        if \"decoder_past_key_value_states\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `decoder_past_key_value_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            past_key_values = kwargs.pop(\"decoder_past_key_value_states\")\n",
    "        if \"decoder_past_key_values\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n",
    "        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n",
    "\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # If decoding with past key value states, only the last tokens\n",
    "        # should be given as an input\n",
    "        if past_key_values is not None:\n",
    "            assert labels is None, \"Decoder should not use cached key value states when training.\"\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "            if decoder_inputs_embeds is not None:\n",
    "                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "        # Rescale output before projecting on vocab\n",
    "        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "        sequence_output = sequence_output * (self.model_dim ** -0.5)\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100, reduction=\"sum\")\n",
    "            loss = []\n",
    "            entropy = []\n",
    "            for i in range(lm_logits.size()[0]):\n",
    "                loss_i = loss_fct(lm_logits[i], labels[i])\n",
    "                ent = calc_entropy(lm_logits[i, 0: decoder_attention_mask[i].sum().item()])\n",
    "                loss.append(loss_i.item())\n",
    "                entropy.append(ent.item())\n",
    "            loss = [loss, entropy]\n",
    "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n",
    "    ):\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        # if decoder past is not included in output\n",
    "        # speedy decoding is disabled and no need to reorder\n",
    "        if past is None:\n",
    "            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n",
    "            return past\n",
    "\n",
    "        reordered_decoder_past = ()\n",
    "        for layer_past_states in past:\n",
    "            # get the correct batch idx from layer past batch dim\n",
    "            # batch dim of `past` is at 2nd position\n",
    "            reordered_layer_past_states = ()\n",
    "            for layer_past_state in layer_past_states:\n",
    "                # need to set correct `past` for each of the four key / value states\n",
    "                reordered_layer_past_states = reordered_layer_past_states + (\n",
    "                    layer_past_state.index_select(0, beam_idx),\n",
    "                )\n",
    "\n",
    "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
    "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
    "\n",
    "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
    "        return reordered_decoder_past\n",
    "\n",
    "# add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING)\n",
    "@add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top. \"\"\", T5_START_DOCSTRING)\n",
    "class MyT5ForConditionalGeneration(T5PreTrainedModel):\n",
    "    authorized_missing_keys = [r\"encoder\\.embed_tokens\\.weight\", r\"decoder\\.embed_tokens\\.weight\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        encoder_config.use_cache = False\n",
    "        encoder_config.is_encoder_decoder = False\n",
    "        self.encoder = T5Stack(encoder_config, self.shared)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.is_encoder_decoder = False\n",
    "        decoder_config.num_layers = config.num_decoder_layers\n",
    "        self.decoder = T5Stack(decoder_config, self.shared)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "        self.encoder.set_input_embeddings(new_embeddings)\n",
    "        self.decoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to ``-100`` are ignored (masked), the loss is only computed for\n",
    "            labels in ``[0, ..., config.vocab_size]``\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\n",
    "\n",
    "            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='pt').input_ids\n",
    "            >>> labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='pt').input_ids\n",
    "            >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "            >>> loss = outputs.loss\n",
    "            >>> logits = outputs.logits\n",
    "\n",
    "            >>> input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you \", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "            >>> outputs = model.generate(input_ids)\n",
    "        \"\"\"\n",
    "\n",
    "        if \"lm_labels\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"lm_labels\")\n",
    "        if \"decoder_past_key_value_states\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `decoder_past_key_value_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            past_key_values = kwargs.pop(\"decoder_past_key_value_states\")\n",
    "        if \"decoder_past_key_values\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n",
    "        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n",
    "\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # If decoding with past key value states, only the last tokens\n",
    "        # should be given as an input\n",
    "        if past_key_values is not None:\n",
    "            assert labels is None, \"Decoder should not use cached key value states when training.\"\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "            if decoder_inputs_embeds is not None:\n",
    "                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "        # Rescale output before projecting on vocab\n",
    "        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "        sequence_output = sequence_output * (self.model_dim ** -0.5)\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            #lm_logits_max, lm_logits_max_index = torch.max(lm_logits, dim=-1)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n",
    "    ):\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        # if decoder past is not included in output\n",
    "        # speedy decoding is disabled and no need to reorder\n",
    "        if past is None:\n",
    "            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n",
    "            return past\n",
    "\n",
    "        reordered_decoder_past = ()\n",
    "        for layer_past_states in past:\n",
    "            # get the correct batch idx from layer past batch dim\n",
    "            # batch dim of `past` is at 2nd position\n",
    "            reordered_layer_past_states = ()\n",
    "            for layer_past_state in layer_past_states:\n",
    "                # need to set correct `past` for each of the four key / value states\n",
    "                reordered_layer_past_states = reordered_layer_past_states + (\n",
    "                    layer_past_state.index_select(0, beam_idx),\n",
    "                )\n",
    "\n",
    "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
    "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
    "\n",
    "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
    "        return reordered_decoder_past\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Fine tune a pre-trained T5 model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, tfm_model, tokenizer, args):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['tfm_model'])\n",
    "        self.config = config\n",
    "        self.model = tfm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "        \n",
    "        self.precompute_tokens()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                labels=None):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(input_ids=batch[\"source_ids\"],\n",
    "                       attention_mask=batch[\"source_mask\"],\n",
    "                       labels=lm_labels,\n",
    "                       decoder_attention_mask=batch['target_mask'])\n",
    "\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        # get f1\n",
    "        outs = self.model.generate(input_ids=batch['source_ids'],\n",
    "                                   attention_mask=batch['source_mask'],\n",
    "                                   max_length=self.config.max_seq_length,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True,\n",
    "                                   num_beams=1)\n",
    "\n",
    "        dec = [\n",
    "            self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in outs.sequences\n",
    "        ]\n",
    "        target = [\n",
    "            self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in batch[\"target_ids\"]\n",
    "        ]\n",
    "        scores, _, _ = compute_scores(dec, target, verbose=False, task=self.args.task)\n",
    "        # f1 = torch.tensor(scores['f1'], dtype=torch.float64)\n",
    "        f1 = torch.tensor(scores[4]['Micro-AVG']['f1'], dtype=torch.float64)\n",
    "        \n",
    "        # get loss\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\",\n",
    "                     loss,\n",
    "                     prog_bar=True,\n",
    "                     on_step=False,\n",
    "                     on_epoch=True)\n",
    "            self.log(f\"{stage}_f1\",\n",
    "                     f1,\n",
    "                     prog_bar=True,\n",
    "                     on_step=False,\n",
    "                     on_epoch=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Prepare optimizer and schedule (linear warmup and decay) \"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                self.config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.config.learning_rate,\n",
    "                          eps=self.config.adam_epsilon)\n",
    "        scheduler = {\n",
    "            \"scheduler\":\n",
    "            get_linear_schedule_with_warmup(optimizer,\n",
    "                                            **self.config.lr_scheduler_init),\n",
    "            \"interval\":\n",
    "            \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        print(\"load training data.\")\n",
    "        train_dataset = ABSADataset(tokenizer=self.tokenizer,\n",
    "                                    task_name=args.task,\n",
    "                                    data_setting=args.data_setting,\n",
    "                                    data_name=args.dataset,\n",
    "                                    language=args.lang,\n",
    "                                    data_type=\"train\",\n",
    "                                    top_k=self.config.top_k,\n",
    "                                    args=self.config,\n",
    "                                    max_len=self.config.max_seq_length)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.train_batch_size,\n",
    "            drop_last=True\n",
    "            if args.data_ratio > 0.3 else False, # don't drop on few-shot\n",
    "            shuffle=True,\n",
    "            num_workers=4)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = ABSADataset(tokenizer=self.tokenizer,\n",
    "                                  task_name=args.task,\n",
    "                                  data_setting=args.data_setting,\n",
    "                                    data_name=args.dataset,\n",
    "                                    language=args.lang,\n",
    "                                    data_type=\"val\",\n",
    "                                  top_k=self.config.num_path,\n",
    "                                  args=self.config,\n",
    "                                  max_len=self.config.max_seq_length)\n",
    "        return DataLoader(val_dataset,\n",
    "                          batch_size=self.config.eval_batch_size,\n",
    "                          num_workers=4)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def rindex(_list, _value):\n",
    "        return len(_list) - _list[::-1].index(_value) - 1\n",
    "\n",
    "    def precompute_tokens(self):\n",
    "        dic = {\"cate_tokens\":{}, \"all_tokens\":{}, \"sentiment_tokens\":{}, 'special_tokens':[]}\n",
    "        for task in force_words.keys():\n",
    "            dic[\"all_tokens\"][task] = {}\n",
    "            for dataset in force_words[task].keys():\n",
    "                cur_list = force_words[task][dataset]\n",
    "                tokenize_res = []\n",
    "                for w in cur_list:\n",
    "                    tokenize_res.extend(self.tokenizer(w, return_tensors='pt')['input_ids'].tolist()[0])\n",
    "                dic[\"all_tokens\"][task][dataset] = tokenize_res\n",
    "        for k,v in cate_list.items():\n",
    "            tokenize_res = []\n",
    "            for w in v:\n",
    "                tokenize_res.extend(self.tokenizer(w, return_tensors='pt')['input_ids'].tolist()[0]) \n",
    "            dic[\"cate_tokens\"][str(k)] = tokenize_res\n",
    "        sp_tokenize_res = []\n",
    "        for sp in ['great', 'ok', 'bad', 'gut', 'ok', 'schlecht']:\n",
    "            sp_tokenize_res.extend(self.tokenizer(sp, return_tensors='pt')['input_ids'].tolist()[0])\n",
    "        for task in force_words.keys():\n",
    "            dic['sentiment_tokens'][str(task)] = sp_tokenize_res\n",
    "        #dic['sentiment_tokens'] = sp_tokenize_res\n",
    "        special_tokens_tokenize_res = []\n",
    "        for w in ['[O','[A','[S','[C','[SS']:\n",
    "            special_tokens_tokenize_res.extend(self.tokenizer(w, return_tensors='pt')['input_ids'].tolist()[0]) \n",
    "        if self.args.model_name_or_path == 'google/mt5-small':\n",
    "            special_tokens_tokenize_res = [r for r in special_tokens_tokenize_res if r != 491]\n",
    "        else:\n",
    "            special_tokens_tokenize_res = [r for r in special_tokens_tokenize_res if r != 784]\n",
    "        dic['special_tokens'] = special_tokens_tokenize_res\n",
    "\n",
    "        self.force_tokens = dic \n",
    "    \n",
    "    def prefix_allowed_tokens_fn(self, task, data_name, source_ids, batch_id,\n",
    "                                 input_ids):\n",
    "        \"\"\"\n",
    "        Constrained Decoding\n",
    "        # ids = self.tokenizer(\"text\", return_tensors='pt')['input_ids'].tolist()[0]\n",
    "        \"\"\"\n",
    "            \n",
    "        force_tokens = self.force_tokens\n",
    "        \n",
    "        # google/mt5-small\n",
    "        to_id = {\n",
    "            'OT': [646],\n",
    "            'AT': [357],\n",
    "            'SP': [399],\n",
    "            'AC': [424],\n",
    "            'SS': [14826],\n",
    "            'EP': [22818],\n",
    "            '[': [491],\n",
    "            ']': [259, 439],\n",
    "            'it': [609],\n",
    "            # 'es': [3, 15, 7],\n",
    "            'null': [259, 1181]\n",
    "        }\n",
    "\n",
    "        left_brace_index = (input_ids == to_id['['][0]).nonzero()\n",
    "        right_brace_index = (input_ids == to_id[']'][0]).nonzero()\n",
    "        num_left_brace = len(left_brace_index)\n",
    "        num_right_brace = len(right_brace_index)\n",
    "        last_right_brace_pos = right_brace_index[-1][\n",
    "            0] if right_brace_index.nelement() > 0 else -1\n",
    "        last_left_brace_pos = left_brace_index[-1][\n",
    "            0] if left_brace_index.nelement() > 0 else -1\n",
    "        cur_id = input_ids[-1]\n",
    "\n",
    "        if cur_id in to_id['[']:\n",
    "            return force_tokens['special_tokens']\n",
    "        elif cur_id in to_id['AT'] + to_id['OT'] + to_id['EP'] + to_id['SP'] + to_id['AC']:  \n",
    "            return to_id[']']  \n",
    "        elif cur_id in to_id['SS']:  \n",
    "            return to_id['EP'] \n",
    "       \n",
    "        # get cur_term\n",
    "        if last_left_brace_pos == -1:\n",
    "            return to_id['['] + [1]   # start of sentence: [\n",
    "        elif (last_left_brace_pos != -1 and last_right_brace_pos == -1) \\\n",
    "            or last_left_brace_pos > last_right_brace_pos:\n",
    "            return to_id[']']  # ]\n",
    "        else:\n",
    "            cur_term = input_ids[last_left_brace_pos + 1]\n",
    "\n",
    "        ret = []\n",
    "        if cur_term in to_id['SP']:  # SP\n",
    "            ret = force_tokens['sentiment_tokens'][str(task)]\n",
    "        elif cur_term in to_id['AT']:  # AT\n",
    "            force_list = source_ids[batch_id].tolist()\n",
    "            if task != 'aste': \n",
    "                if True:\n",
    "                # if data_name == 'rest-16':\n",
    "                    force_list.extend(to_id['it'] + [1])  \n",
    "                # elif data_name == 'GERestaurant':\n",
    "                #     force_list.extend(to_id['es'] + [1])  \n",
    "            ret = force_list  \n",
    "        elif cur_term in to_id['SS']:\n",
    "            ret = [3] + to_id[']'] + [1]\n",
    "        elif cur_term in to_id['AC']:  # AC\n",
    "            ret = force_tokens['cate_tokens'][str(data_name)]\n",
    "        elif cur_term in to_id['OT']:  # OT\n",
    "            force_list = source_ids[batch_id].tolist()\n",
    "            if task == \"acos\":\n",
    "                force_list.extend(to_id['null'])  # null\n",
    "            ret = force_list\n",
    "        else:\n",
    "            raise ValueError(cur_term)    \n",
    "\n",
    "        if num_left_brace == num_right_brace:\n",
    "            ret = set(ret)\n",
    "            ret.discard(to_id[']'][0]) # remove ]\n",
    "            for w in force_tokens['special_tokens']:\n",
    "                ret.discard(w)\n",
    "            ret = list(ret)\n",
    "        elif num_left_brace > num_right_brace:\n",
    "            ret += to_id[']'] \n",
    "        else:\n",
    "            raise ValueError\n",
    "        ret.extend(to_id['['] + [1]) # add [\n",
    "        return ret\n",
    "\n",
    "\n",
    "def evaluate(model, task, lang, data_setting, dataset, args, data_type):\n",
    "    \"\"\"\n",
    "    Compute scores given the predictions and gold labels\n",
    "    \"\"\"\n",
    "\n",
    "    outputs, targets, probs = [], [], []\n",
    "    num_path = args.num_path\n",
    "    \n",
    "    if task in ['aste', 'tasd']:\n",
    "        num_path = min(5, num_path)\n",
    "        \n",
    "    dataset = ABSADataset(tokenizer=model.tokenizer,\n",
    "                                  task_name=task,\n",
    "                                  data_setting=data_setting,\n",
    "                                    data_name=dataset,\n",
    "                                    language=lang,\n",
    "                                    data_type=data_type,\n",
    "                                  top_k=num_path,\n",
    "                                  args=args,\n",
    "                                  max_len=args.max_seq_length)\n",
    "    \n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=args.eval_batch_size,\n",
    "                             num_workers=2)\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    model.model.to(device)\n",
    "    model.model.eval()\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        # beam search\n",
    "\n",
    "        outs = model.model.generate(\n",
    "            input_ids=batch['source_ids'].to(device),\n",
    "            attention_mask=batch['source_mask'].to(device),\n",
    "            max_length=args.max_seq_length,\n",
    "            num_beams=args.beam_size,\n",
    "            early_stopping=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            prefix_allowed_tokens_fn=partial(\n",
    "               model.prefix_allowed_tokens_fn, task, args.dataset,\n",
    "               batch['source_ids']) if args.constrained_decode else None,\n",
    "        )\n",
    "        dec = [\n",
    "            model.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in outs.sequences\n",
    "        ]\n",
    "        target = [\n",
    "            model.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in batch[\"target_ids\"]\n",
    "        ]\n",
    "        outputs.extend(dec)\n",
    "        targets.extend(target)\n",
    "\n",
    "    if args.multi_path:\n",
    "        targets = targets[::num_path]\n",
    "\n",
    "        # get outputs\n",
    "        _outputs = outputs # backup\n",
    "        outputs = [] # new outputs\n",
    "        if args.agg_strategy == 'post_rank':\n",
    "            inputs = [ele for ele in sents for _ in range(num_path)]\n",
    "            assert len(_outputs) == len(inputs), (len(_outputs), len(inputs))\n",
    "            preds = [[o] for o in _outputs] \n",
    "            model_path = os.path.join(args.output_dir, \"final\")\n",
    "            scores = cal_entropy(inputs, preds, model_path, model.tokenizer)\n",
    "\n",
    "        for i in range(0, len(targets)):\n",
    "            o_idx = i * num_path\n",
    "            multi_outputs = _outputs[o_idx:o_idx + num_path]\n",
    "\n",
    "            if args.agg_strategy == 'post_rank':\n",
    "                multi_probs = scores[o_idx:o_idx + args.num_path]\n",
    "                assert len(multi_outputs) == len(multi_probs)\n",
    "\n",
    "                sorted_outputs = [i for _,i in sorted(zip(multi_probs,multi_outputs))]\n",
    "                outputs.append(sorted_outputs[0])\n",
    "                continue\n",
    "            elif args.agg_strategy == \"pre_rank\":\n",
    "                outputs.append(multi_outputs[0])\n",
    "                continue\n",
    "            elif args.agg_strategy == 'rand':\n",
    "                outputs.append(random.choice(multi_outputs))\n",
    "                continue\n",
    "            elif args.agg_strategy == 'vote':\n",
    "                all_quads = []\n",
    "                for s in multi_outputs:\n",
    "                    all_quads.extend(\n",
    "                        extract_spans_para(seq=s, seq_type='pred'))\n",
    "\n",
    "                output_quads = []\n",
    "                counter = dict(Counter(all_quads))\n",
    "                for quad, count in counter.items():\n",
    "                    # keep freq >= num_path / 2\n",
    "                    if count >= len(multi_outputs) / 2:\n",
    "                        output_quads.append(quad)\n",
    "\n",
    "                # recover output\n",
    "                output = []\n",
    "                for q in output_quads:\n",
    "                    ac, at, sp, ot = q\n",
    "                    if task == \"aste\":\n",
    "                        if 'null' not in [at, ot, sp]:  # aste has no 'null', for zero-shot only\n",
    "                            output.append(f'[A] {at} [O] {ot} [S] {sp}')\n",
    "\n",
    "                    elif task  == \"tasd\":\n",
    "                        output.append(f\"[A] {at} [S] {sp} [C] {ac}\")\n",
    "\n",
    "                    elif task in [\"asqp\", \"acos\"]:\n",
    "                        output.append(f\"[A] {at} [O] {ot} [S] {sp} [C] {ac}\")\n",
    "\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "\n",
    "                target_quads = extract_spans_para(seq=targets[i],\n",
    "                                                seq_type='gold')\n",
    "\n",
    "                # if no output, use the first path\n",
    "                output_str = \" [SSEP] \".join(\n",
    "                    output) if output else multi_outputs[0]\n",
    "\n",
    "                outputs.append(output_str)\n",
    "\n",
    "    # stats\n",
    "    labels_counts = Counter([len(l.split('[SSEP]')) for l in outputs])\n",
    "\n",
    "    print('After Prediction')\n",
    "    print('Preds')\n",
    "    print(outputs[:5])\n",
    "    print('Golds')\n",
    "    print(targets[:5])\n",
    "    \n",
    "    scores, all_labels, preds = compute_scores(outputs,\n",
    "                                                   targets,\n",
    "                                                   verbose=True, task=args.task)\n",
    "    return scores, preds\n",
    "\n",
    "\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "def signal_handler(signal, frame):\n",
    "    print(\"Training interrupted by user!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "\n",
    "set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "723f03a0-f4e1-4f67-ab0c-96242a30b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace()\n",
    "\n",
    "args.model_name_or_path = 'google/mt5-small'\n",
    "args.lang = 'en'\n",
    "args.data_path = '../../../data/restaurant/'\n",
    "args.lang_setting = 'orig'\n",
    "args.eval_type = 'test'\n",
    "args.data_setting = 'orig'\n",
    "args.output_dir = 'results/'\n",
    "args.task = 'tasd'\n",
    "args.num_train_epochs = 30\n",
    "args.save_top_k = 0\n",
    "args.top_k = 5\n",
    "args.ctrl_token = 'post' # Ändern?\n",
    "args.multi_path = True\n",
    "args.num_path = 5\n",
    "args.lowercase = True\n",
    "args.train_batch_size = 8\n",
    "args.gradient_accumulation_steps = 2\n",
    "args.learning_rate = 1e-4\n",
    "args.sort_label = True\n",
    "args.agg_strategy = 'vote'\n",
    "args.data_ratio = 1.0\n",
    "args.eval_batch_size = 16\n",
    "args.constrained_decode = True\n",
    "args.do_train = True\n",
    "args.max_seq_length = 200\n",
    "args.multi_task = False\n",
    "args.single_view_type = 'rank'\n",
    "args.check_val_every_n_epoch = 20\n",
    "args.load_path_cache = False\n",
    "args.beam_size = 1\n",
    "args.warmup_steps = 0\n",
    "args.adam_epsilon = 1e-8\n",
    "args.weight_decay = 0\n",
    "args.n_gpu = 1\n",
    "args.load_ckpt_name = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e65014a2-ad73-4578-8ce8-70880fd7cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Python, Pytorch, Cuda info____\n",
      "__Python VERSION: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "__pyTorch VERSION: 2.1.2+cu121\n",
      "__CUDA RUNTIME API VERSION\n",
      "__CUDNN VERSION: 8902\n",
      "_____nvidia-smi GPU details____\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA RTX A5000, 525.147.05, 24564 MiB, 3 MiB, 24243 MiB\n",
      "1, NVIDIA RTX A5000, 525.147.05, 24564 MiB, 10901 MiB, 13337 MiB\n",
      "_____Device assignments____\n",
      "Number CUDA Devices: 1\n",
      "Current cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n",
      "Device name:  NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from subprocess import call\n",
    "print('_____Python, Pytorch, Cuda info____')\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA RUNTIME API VERSION')\n",
    "#os.system('nvcc --version')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('_____nvidia-smi GPU details____')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('_____Device assignments____')\n",
    "print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "print ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')\n",
    "print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ae98e-76e6-4481-b3ae-1fb5cbfff628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Conduct Training ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load training data.\n",
      "Loading Train Dataset\n",
      "Total examples = 1708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708 8540 8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | MyT5ForConditionalGeneration | 300 M \n",
      "-------------------------------------------------------\n",
      "300 M     Trainable params\n",
      "0         Non-trainable params\n",
      "300 M     Total params\n",
      "1,200.707 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation Dataset\n",
      "Total examples = 587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587 587 587\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "load training data.\n",
      "Loading Train Dataset\n",
      "Total examples = 1708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708 8540 8540\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d312c79b0a48ae9f3e6100fdb42b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved. New best score: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  5.555555555555555\n",
      "MVP F1-Micro:  5.714285714285714\n",
      "MVP F1-Micro:  3.92156862745098\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  4.705882352941177\n",
      "MVP F1-Micro:  6.896551724137931\n",
      "MVP F1-Micro:  10.714285714285714\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  4.545454545454546\n",
      "MVP F1-Micro:  13.333333333333334\n",
      "MVP F1-Micro:  5.194805194805195\n",
      "MVP F1-Micro:  5.2631578947368425\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  5.555555555555556\n",
      "MVP F1-Micro:  19.04761904761905\n",
      "MVP F1-Micro:  15.789473684210526\n",
      "MVP F1-Micro:  3.3898305084745757\n",
      "MVP F1-Micro:  4.651162790697675\n",
      "MVP F1-Micro:  4.347826086956522\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  3.3333333333333335\n",
      "MVP F1-Micro:  20.0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  3.3333333333333335\n",
      "MVP F1-Micro:  1.9230769230769231\n",
      "MVP F1-Micro:  3.4782608695652173\n",
      "MVP F1-Micro:  1.8691588785046727\n",
      "MVP F1-Micro:  13.114754098360654\n",
      "MVP F1-Micro:  2.4691358024691357\n",
      "MVP F1-Micro:  6.349206349206349\n",
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.047 >= min_delta = 0.0. New best score: 0.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  22.22222222222222\n",
      "MVP F1-Micro:  10.526315789473683\n",
      "MVP F1-Micro:  20.634920634920633\n",
      "MVP F1-Micro:  4.705882352941176\n",
      "MVP F1-Micro:  29.78723404255319\n",
      "MVP F1-Micro:  18.6046511627907\n",
      "MVP F1-Micro:  21.276595744680854\n",
      "MVP F1-Micro:  6.521739130434782\n",
      "MVP F1-Micro:  16.3265306122449\n",
      "MVP F1-Micro:  21.21212121212121\n",
      "MVP F1-Micro:  9.876543209876543\n",
      "MVP F1-Micro:  9.836065573770494\n",
      "MVP F1-Micro:  8.333333333333334\n",
      "MVP F1-Micro:  31.034482758620697\n",
      "MVP F1-Micro:  22.78481012658228\n",
      "MVP F1-Micro:  13.043478260869565\n",
      "MVP F1-Micro:  24.390243902439025\n",
      "MVP F1-Micro:  10.526315789473685\n",
      "MVP F1-Micro:  25.641025641025646\n",
      "MVP F1-Micro:  9.6\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  6.451612903225808\n",
      "MVP F1-Micro:  8.51063829787234\n",
      "MVP F1-Micro:  8.333333333333332\n",
      "MVP F1-Micro:  4.166666666666666\n",
      "MVP F1-Micro:  0\n",
      "MVP F1-Micro:  6.153846153846154\n",
      "MVP F1-Micro:  9.00900900900901\n",
      "MVP F1-Micro:  16.129032258064516\n",
      "MVP F1-Micro:  9.523809523809522\n",
      "MVP F1-Micro:  15.384615384615385\n",
      "MVP F1-Micro:  9.836065573770494\n",
      "MVP F1-Micro:  41.55844155844156\n",
      "MVP F1-Micro:  13.636363636363635\n",
      "MVP F1-Micro:  12.5\n",
      "MVP F1-Micro:  1.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.079 >= min_delta = 0.0. New best score: 0.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  35.294117647058826\n",
      "MVP F1-Micro:  16.666666666666668\n",
      "MVP F1-Micro:  50.70422535211269\n",
      "MVP F1-Micro:  4.25531914893617\n",
      "MVP F1-Micro:  34.146341463414636\n",
      "MVP F1-Micro:  25.641025641025646\n",
      "MVP F1-Micro:  34.14634146341463\n",
      "MVP F1-Micro:  17.02127659574468\n",
      "MVP F1-Micro:  13.636363636363638\n",
      "MVP F1-Micro:  20.833333333333336\n",
      "MVP F1-Micro:  40.909090909090914\n",
      "MVP F1-Micro:  15.0\n",
      "MVP F1-Micro:  10.0\n",
      "MVP F1-Micro:  48.78048780487806\n",
      "MVP F1-Micro:  55.319148936170215\n",
      "MVP F1-Micro:  25.641025641025646\n",
      "MVP F1-Micro:  27.77777777777778\n",
      "MVP F1-Micro:  14.545454545454547\n",
      "MVP F1-Micro:  27.58620689655172\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  15.0\n",
      "MVP F1-Micro:  10.256410256410255\n",
      "MVP F1-Micro:  15.384615384615383\n",
      "MVP F1-Micro:  18.18181818181818\n",
      "MVP F1-Micro:  5.0\n",
      "MVP F1-Micro:  4.761904761904763\n",
      "MVP F1-Micro:  11.538461538461538\n",
      "MVP F1-Micro:  59.70149253731344\n",
      "MVP F1-Micro:  23.076923076923077\n",
      "MVP F1-Micro:  23.809523809523807\n",
      "MVP F1-Micro:  31.57894736842105\n",
      "MVP F1-Micro:  34.61538461538461\n",
      "MVP F1-Micro:  59.375\n",
      "MVP F1-Micro:  22.22222222222222\n",
      "MVP F1-Micro:  35.55555555555556\n",
      "MVP F1-Micro:  3.773584905660378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.077 >= min_delta = 0.0. New best score: 0.203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  52.94117647058824\n",
      "MVP F1-Micro:  21.62162162162162\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  30.434782608695656\n",
      "MVP F1-Micro:  46.51162790697674\n",
      "MVP F1-Micro:  41.02564102564102\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  25.531914893617024\n",
      "MVP F1-Micro:  13.636363636363638\n",
      "MVP F1-Micro:  11.76470588235294\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  14.814814814814813\n",
      "MVP F1-Micro:  23.25581395348837\n",
      "MVP F1-Micro:  58.536585365853654\n",
      "MVP F1-Micro:  48.275862068965516\n",
      "MVP F1-Micro:  39.02439024390244\n",
      "MVP F1-Micro:  22.222222222222225\n",
      "MVP F1-Micro:  18.18181818181818\n",
      "MVP F1-Micro:  28.070175438596497\n",
      "MVP F1-Micro:  34.04255319148936\n",
      "MVP F1-Micro:  12.76595744680851\n",
      "MVP F1-Micro:  29.268292682926827\n",
      "MVP F1-Micro:  15.0\n",
      "MVP F1-Micro:  48.387096774193544\n",
      "MVP F1-Micro:  5.128205128205128\n",
      "MVP F1-Micro:  4.545454545454545\n",
      "MVP F1-Micro:  23.076923076923077\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  35.8974358974359\n",
      "MVP F1-Micro:  28.571428571428566\n",
      "MVP F1-Micro:  30.76923076923077\n",
      "MVP F1-Micro:  15.384615384615383\n",
      "MVP F1-Micro:  70.0\n",
      "MVP F1-Micro:  22.22222222222222\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  3.7037037037037033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.032 >= min_delta = 0.0. New best score: 0.235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  8.51063829787234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  34.285714285714285\n",
      "MVP F1-Micro:  59.45945945945946\n",
      "MVP F1-Micro:  57.14285714285714\n",
      "MVP F1-Micro:  68.0\n",
      "MVP F1-Micro:  59.57446808510638\n",
      "MVP F1-Micro:  53.658536585365844\n",
      "MVP F1-Micro:  31.818181818181817\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  32.0\n",
      "MVP F1-Micro:  23.52941176470588\n",
      "MVP F1-Micro:  34.04255319148936\n",
      "MVP F1-Micro:  22.727272727272727\n",
      "MVP F1-Micro:  44.44444444444445\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  34.14634146341463\n",
      "MVP F1-Micro:  55.00000000000001\n",
      "MVP F1-Micro:  21.428571428571427\n",
      "MVP F1-Micro:  33.898305084745765\n",
      "MVP F1-Micro:  38.297872340425535\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  25.0\n",
      "MVP F1-Micro:  19.047619047619047\n",
      "MVP F1-Micro:  13.33333333333333\n",
      "MVP F1-Micro:  31.818181818181817\n",
      "MVP F1-Micro:  49.999999999999986\n",
      "MVP F1-Micro:  36.36363636363636\n",
      "MVP F1-Micro:  31.111111111111118\n",
      "MVP F1-Micro:  31.57894736842105\n",
      "MVP F1-Micro:  60.465116279069775\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  30.76923076923077\n",
      "MVP F1-Micro:  36.734693877551024\n",
      "MVP F1-Micro:  3.448275862068965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.114 >= min_delta = 0.0. New best score: 0.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  9.75609756097561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  65.21739130434783\n",
      "MVP F1-Micro:  62.22222222222222\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  56.41025641025642\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  48.0\n",
      "MVP F1-Micro:  45.83333333333333\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  23.809523809523807\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  65.11627906976744\n",
      "MVP F1-Micro:  51.06382978723405\n",
      "MVP F1-Micro:  30.0\n",
      "MVP F1-Micro:  56.41025641025641\n",
      "MVP F1-Micro:  21.428571428571427\n",
      "MVP F1-Micro:  39.28571428571428\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  60.0\n",
      "MVP F1-Micro:  39.02439024390244\n",
      "MVP F1-Micro:  22.222222222222225\n",
      "MVP F1-Micro:  24.390243902439025\n",
      "MVP F1-Micro:  18.6046511627907\n",
      "MVP F1-Micro:  34.14634146341463\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  38.095238095238095\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  60.465116279069775\n",
      "MVP F1-Micro:  68.29268292682926\n",
      "MVP F1-Micro:  32.43243243243243\n",
      "MVP F1-Micro:  37.5\n",
      "MVP F1-Micro:  3.846153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.064 >= min_delta = 0.0. New best score: 0.413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  10.526315789473685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  59.45945945945946\n",
      "MVP F1-Micro:  73.91304347826087\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  72.72727272727272\n",
      "MVP F1-Micro:  56.41025641025642\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  48.97959183673469\n",
      "MVP F1-Micro:  45.83333333333333\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  41.86046511627907\n",
      "MVP F1-Micro:  61.904761904761905\n",
      "MVP F1-Micro:  55.81395348837209\n",
      "MVP F1-Micro:  63.829787234042556\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  51.28205128205129\n",
      "MVP F1-Micro:  25.53191489361702\n",
      "MVP F1-Micro:  35.71428571428571\n",
      "MVP F1-Micro:  52.17391304347826\n",
      "MVP F1-Micro:  54.05405405405405\n",
      "MVP F1-Micro:  58.536585365853654\n",
      "MVP F1-Micro:  38.095238095238095\n",
      "MVP F1-Micro:  22.727272727272727\n",
      "MVP F1-Micro:  29.268292682926827\n",
      "MVP F1-Micro:  23.25581395348837\n",
      "MVP F1-Micro:  35.0\n",
      "MVP F1-Micro:  45.454545454545446\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  54.54545454545454\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  55.81395348837208\n",
      "MVP F1-Micro:  72.72727272727273\n",
      "MVP F1-Micro:  30.76923076923077\n",
      "MVP F1-Micro:  45.83333333333333\n",
      "MVP F1-Micro:  7.692307692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.021 >= min_delta = 0.0. New best score: 0.434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  15.384615384615383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  48.64864864864865\n",
      "MVP F1-Micro:  89.36170212765957\n",
      "MVP F1-Micro:  74.99999999999999\n",
      "MVP F1-Micro:  73.91304347826086\n",
      "MVP F1-Micro:  56.41025641025642\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  48.97959183673469\n",
      "MVP F1-Micro:  35.99999999999999\n",
      "MVP F1-Micro:  24.000000000000004\n",
      "MVP F1-Micro:  54.54545454545454\n",
      "MVP F1-Micro:  38.0952380952381\n",
      "MVP F1-Micro:  61.904761904761905\n",
      "MVP F1-Micro:  62.22222222222222\n",
      "MVP F1-Micro:  76.0\n",
      "MVP F1-Micro:  58.536585365853654\n",
      "MVP F1-Micro:  56.41025641025641\n",
      "MVP F1-Micro:  32.6530612244898\n",
      "MVP F1-Micro:  35.08771929824561\n",
      "MVP F1-Micro:  60.86956521739131\n",
      "MVP F1-Micro:  57.89473684210527\n",
      "MVP F1-Micro:  63.41463414634146\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  31.11111111111111\n",
      "MVP F1-Micro:  33.33333333333333\n",
      "MVP F1-Micro:  31.818181818181824\n",
      "MVP F1-Micro:  33.33333333333333\n",
      "MVP F1-Micro:  53.33333333333332\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  53.33333333333332\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  54.54545454545454\n",
      "MVP F1-Micro:  73.91304347826087\n",
      "MVP F1-Micro:  45.0\n",
      "MVP F1-Micro:  44.89795918367347\n",
      "MVP F1-Micro:  11.320754716981133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.020 >= min_delta = 0.0. New best score: 0.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  9.75609756097561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  82.6086956521739\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  73.91304347826086\n",
      "MVP F1-Micro:  55.000000000000014\n",
      "MVP F1-Micro:  31.818181818181817\n",
      "MVP F1-Micro:  48.0\n",
      "MVP F1-Micro:  36.73469387755102\n",
      "MVP F1-Micro:  28.000000000000004\n",
      "MVP F1-Micro:  56.52173913043478\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  43.90243902439025\n",
      "MVP F1-Micro:  59.09090909090908\n",
      "MVP F1-Micro:  62.499999999999986\n",
      "MVP F1-Micro:  43.90243902439025\n",
      "MVP F1-Micro:  51.28205128205129\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  38.59649122807017\n",
      "MVP F1-Micro:  57.777777777777786\n",
      "MVP F1-Micro:  57.89473684210527\n",
      "MVP F1-Micro:  63.41463414634146\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  22.727272727272727\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  36.36363636363636\n",
      "MVP F1-Micro:  33.33333333333333\n",
      "MVP F1-Micro:  44.44444444444445\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  31.57894736842105\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  65.21739130434783\n",
      "MVP F1-Micro:  55.000000000000014\n",
      "MVP F1-Micro:  48.97959183673469\n",
      "MVP F1-Micro:  11.320754716981133\n",
      "MVP F1-Micro:  15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  89.79591836734694\n",
      "MVP F1-Micro:  73.46938775510205\n",
      "MVP F1-Micro:  76.59574468085107\n",
      "MVP F1-Micro:  55.000000000000014\n",
      "MVP F1-Micro:  35.55555555555555\n",
      "MVP F1-Micro:  48.0\n",
      "MVP F1-Micro:  44.89795918367348\n",
      "MVP F1-Micro:  26.229508196721312\n",
      "MVP F1-Micro:  56.52173913043478\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  55.81395348837208\n",
      "MVP F1-Micro:  62.22222222222222\n",
      "MVP F1-Micro:  73.46938775510203\n",
      "MVP F1-Micro:  43.90243902439025\n",
      "MVP F1-Micro:  51.28205128205129\n",
      "MVP F1-Micro:  35.08771929824562\n",
      "MVP F1-Micro:  38.59649122807017\n",
      "MVP F1-Micro:  65.21739130434783\n",
      "MVP F1-Micro:  63.1578947368421\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  21.73913043478261\n",
      "MVP F1-Micro:  38.095238095238095\n",
      "MVP F1-Micro:  22.727272727272723\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  46.80851063829787\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  46.15384615384615\n",
      "MVP F1-Micro:  55.81395348837208\n",
      "MVP F1-Micro:  75.55555555555556\n",
      "MVP F1-Micro:  46.15384615384615\n",
      "MVP F1-Micro:  40.81632653061225\n",
      "MVP F1-Micro:  11.111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.009 >= min_delta = 0.0. New best score: 0.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  62.85714285714287\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  89.79591836734694\n",
      "MVP F1-Micro:  70.83333333333334\n",
      "MVP F1-Micro:  70.83333333333334\n",
      "MVP F1-Micro:  58.536585365853654\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  39.21568627450981\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  29.629629629629633\n",
      "MVP F1-Micro:  57.777777777777786\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  69.3877551020408\n",
      "MVP F1-Micro:  53.65853658536586\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  42.30769230769231\n",
      "MVP F1-Micro:  38.59649122807017\n",
      "MVP F1-Micro:  59.57446808510638\n",
      "MVP F1-Micro:  68.42105263157895\n",
      "MVP F1-Micro:  74.4186046511628\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  29.166666666666668\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  34.78260869565217\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  42.10526315789473\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  79.16666666666666\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  44.89795918367347\n",
      "MVP F1-Micro:  14.545454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.015 >= min_delta = 0.0. New best score: 0.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  16.666666666666664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  83.33333333333334\n",
      "MVP F1-Micro:  70.83333333333334\n",
      "MVP F1-Micro:  79.16666666666667\n",
      "MVP F1-Micro:  53.658536585365844\n",
      "MVP F1-Micro:  31.818181818181817\n",
      "MVP F1-Micro:  47.05882352941176\n",
      "MVP F1-Micro:  40.816326530612244\n",
      "MVP F1-Micro:  33.9622641509434\n",
      "MVP F1-Micro:  62.222222222222236\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  71.11111111111111\n",
      "MVP F1-Micro:  69.3877551020408\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  39.28571428571428\n",
      "MVP F1-Micro:  68.08510638297872\n",
      "MVP F1-Micro:  61.53846153846153\n",
      "MVP F1-Micro:  61.904761904761905\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  36.36363636363636\n",
      "MVP F1-Micro:  38.297872340425535\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  55.319148936170215\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  52.17391304347826\n",
      "MVP F1-Micro:  42.10526315789473\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  82.6086956521739\n",
      "MVP F1-Micro:  46.15384615384615\n",
      "MVP F1-Micro:  48.97959183673469\n",
      "MVP F1-Micro:  11.111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.003 >= min_delta = 0.0. New best score: 0.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  19.51219512195122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  64.86486486486486\n",
      "MVP F1-Micro:  85.71428571428572\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  79.16666666666667\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  57.6923076923077\n",
      "MVP F1-Micro:  42.30769230769231\n",
      "MVP F1-Micro:  33.9622641509434\n",
      "MVP F1-Micro:  59.09090909090909\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  43.13725490196078\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  70.83333333333334\n",
      "MVP F1-Micro:  70.27027027027027\n",
      "MVP F1-Micro:  69.76744186046511\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  24.489795918367346\n",
      "MVP F1-Micro:  41.86046511627907\n",
      "MVP F1-Micro:  43.47826086956522\n",
      "MVP F1-Micro:  36.36363636363637\n",
      "MVP F1-Micro:  55.319148936170215\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  42.10526315789473\n",
      "MVP F1-Micro:  60.465116279069775\n",
      "MVP F1-Micro:  85.10638297872342\n",
      "MVP F1-Micro:  56.41025641025641\n",
      "MVP F1-Micro:  53.06122448979592\n",
      "MVP F1-Micro:  14.814814814814813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.018 >= min_delta = 0.0. New best score: 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  20.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  75.67567567567566\n",
      "MVP F1-Micro:  89.79591836734694\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  80.85106382978724\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  36.36363636363637\n",
      "MVP F1-Micro:  53.84615384615385\n",
      "MVP F1-Micro:  47.05882352941176\n",
      "MVP F1-Micro:  38.46153846153847\n",
      "MVP F1-Micro:  55.81395348837209\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  69.3877551020408\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  70.27027027027027\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  28.57142857142857\n",
      "MVP F1-Micro:  46.51162790697674\n",
      "MVP F1-Micro:  40.909090909090914\n",
      "MVP F1-Micro:  32.558139534883715\n",
      "MVP F1-Micro:  55.319148936170215\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  80.85106382978724\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  48.00000000000001\n",
      "MVP F1-Micro:  18.51851851851852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.010 >= min_delta = 0.0. New best score: 0.509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  20.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  57.14285714285715\n",
      "MVP F1-Micro:  75.67567567567566\n",
      "MVP F1-Micro:  93.87755102040816\n",
      "MVP F1-Micro:  65.3061224489796\n",
      "MVP F1-Micro:  76.59574468085107\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  40.0\n",
      "MVP F1-Micro:  56.60377358490566\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  45.28301886792453\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  53.33333333333332\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  71.99999999999999\n",
      "MVP F1-Micro:  43.90243902439025\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  43.99999999999999\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  73.68421052631578\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  24.489795918367346\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  40.909090909090914\n",
      "MVP F1-Micro:  31.818181818181817\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  51.162790697674424\n",
      "MVP F1-Micro:  52.17391304347826\n",
      "MVP F1-Micro:  41.02564102564102\n",
      "MVP F1-Micro:  55.81395348837208\n",
      "MVP F1-Micro:  82.6086956521739\n",
      "MVP F1-Micro:  51.28205128205129\n",
      "MVP F1-Micro:  53.06122448979592\n",
      "MVP F1-Micro:  21.81818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.004 >= min_delta = 0.0. New best score: 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  23.809523809523807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  75.67567567567566\n",
      "MVP F1-Micro:  83.33333333333334\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  73.91304347826086\n",
      "MVP F1-Micro:  63.41463414634146\n",
      "MVP F1-Micro:  53.33333333333332\n",
      "MVP F1-Micro:  50.98039215686274\n",
      "MVP F1-Micro:  47.05882352941176\n",
      "MVP F1-Micro:  41.50943396226415\n",
      "MVP F1-Micro:  60.46511627906976\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  61.904761904761905\n",
      "MVP F1-Micro:  71.11111111111111\n",
      "MVP F1-Micro:  68.08510638297872\n",
      "MVP F1-Micro:  43.90243902439025\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  44.89795918367347\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  68.08510638297872\n",
      "MVP F1-Micro:  78.94736842105262\n",
      "MVP F1-Micro:  69.76744186046511\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  24.489795918367346\n",
      "MVP F1-Micro:  42.857142857142854\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  53.658536585365844\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  47.368421052631575\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  84.44444444444444\n",
      "MVP F1-Micro:  56.41025641025641\n",
      "MVP F1-Micro:  53.06122448979592\n",
      "MVP F1-Micro:  21.81818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_f1 improved by 0.017 >= min_delta = 0.0. New best score: 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  20.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  51.42857142857144\n",
      "MVP F1-Micro:  75.67567567567566\n",
      "MVP F1-Micro:  85.71428571428572\n",
      "MVP F1-Micro:  62.500000000000014\n",
      "MVP F1-Micro:  69.56521739130434\n",
      "MVP F1-Micro:  53.658536585365844\n",
      "MVP F1-Micro:  45.45454545454545\n",
      "MVP F1-Micro:  48.0\n",
      "MVP F1-Micro:  46.15384615384615\n",
      "MVP F1-Micro:  46.15384615384615\n",
      "MVP F1-Micro:  60.46511627906976\n",
      "MVP F1-Micro:  46.51162790697674\n",
      "MVP F1-Micro:  57.14285714285713\n",
      "MVP F1-Micro:  71.11111111111111\n",
      "MVP F1-Micro:  66.66666666666667\n",
      "MVP F1-Micro:  45.0\n",
      "MVP F1-Micro:  66.66666666666667\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  42.10526315789474\n",
      "MVP F1-Micro:  72.3404255319149\n",
      "MVP F1-Micro:  76.92307692307692\n",
      "MVP F1-Micro:  58.536585365853654\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  36.734693877551024\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  36.36363636363637\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  48.78048780487805\n",
      "MVP F1-Micro:  56.52173913043478\n",
      "MVP F1-Micro:  36.84210526315789\n",
      "MVP F1-Micro:  65.11627906976743\n",
      "MVP F1-Micro:  86.95652173913044\n",
      "MVP F1-Micro:  56.41025641025641\n",
      "MVP F1-Micro:  50.0\n",
      "MVP F1-Micro:  14.814814814814813\n",
      "MVP F1-Micro:  25.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  57.14285714285715\n",
      "MVP F1-Micro:  75.67567567567566\n",
      "MVP F1-Micro:  85.71428571428572\n",
      "MVP F1-Micro:  64.0\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  63.63636363636365\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  51.85185185185185\n",
      "MVP F1-Micro:  47.27272727272727\n",
      "MVP F1-Micro:  46.42857142857143\n",
      "MVP F1-Micro:  55.81395348837209\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  55.81395348837208\n",
      "MVP F1-Micro:  71.11111111111111\n",
      "MVP F1-Micro:  74.50980392156863\n",
      "MVP F1-Micro:  47.61904761904761\n",
      "MVP F1-Micro:  66.66666666666667\n",
      "MVP F1-Micro:  50.98039215686274\n",
      "MVP F1-Micro:  44.827586206896555\n",
      "MVP F1-Micro:  66.66666666666666\n",
      "MVP F1-Micro:  76.92307692307692\n",
      "MVP F1-Micro:  74.4186046511628\n",
      "MVP F1-Micro:  52.38095238095239\n",
      "MVP F1-Micro:  23.52941176470588\n",
      "MVP F1-Micro:  48.888888888888886\n",
      "MVP F1-Micro:  47.82608695652174\n",
      "MVP F1-Micro:  37.2093023255814\n",
      "MVP F1-Micro:  51.06382978723404\n",
      "MVP F1-Micro:  41.86046511627907\n",
      "MVP F1-Micro:  60.86956521739131\n",
      "MVP F1-Micro:  41.02564102564102\n",
      "MVP F1-Micro:  60.465116279069775\n",
      "MVP F1-Micro:  80.85106382978724\n",
      "MVP F1-Micro:  61.53846153846154\n",
      "MVP F1-Micro:  53.06122448979592\n",
      "MVP F1-Micro:  17.543859649122805\n",
      "MVP F1-Micro:  24.390243902439025\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)\n",
    "print(\"\\n****** Conduct Training ******\")\n",
    "\n",
    "# for token in ['OT','AT',\n",
    "#     'SP',\n",
    "#     'AC',\n",
    "#     'SS',\n",
    "#     'EP',\n",
    "#     '[',\n",
    "#     ']',\n",
    "#     'it',\n",
    "#     'null']:\n",
    "#     print(token, tokenizer(token, return_tensors='pt')['input_ids'].tolist())\n",
    "args.dataset = f'rest-{args.lang}'\n",
    "\n",
    "# initialize the T5 model\n",
    "tfm_model = MyT5ForConditionalGeneration.from_pretrained(args.model_name_or_path)\n",
    "model = T5FineTuner(args, tfm_model, tokenizer, args)\n",
    "\n",
    "# load data\n",
    "train_loader = model.train_dataloader()\n",
    "\n",
    "# config optimizer\n",
    "t_total = ((len(train_loader.dataset) //\n",
    "            (args.train_batch_size * max(1, args.n_gpu))) //\n",
    "           args.gradient_accumulation_steps *\n",
    "           float(args.num_train_epochs))\n",
    "\n",
    "args.lr_scheduler_init = {\n",
    "    \"num_warmup_steps\": args.warmup_steps,\n",
    "    \"num_training_steps\": t_total\n",
    "}\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir,\n",
    "    filename='{epoch}-{val_f1:.2f}-{val_loss:.2f}',\n",
    "    monitor='val_f1',\n",
    "    mode='max',\n",
    "    save_top_k=args.save_top_k,\n",
    "    save_last=False)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_f1\",\n",
    "                                    min_delta=0.00,\n",
    "                                    patience=20,\n",
    "                                    verbose=True,\n",
    "                                    mode=\"max\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# prepare for trainer\n",
    "train_params = dict(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    default_root_dir=args.output_dir,\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gradient_clip_val=1.0,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[\n",
    "        checkpoint_callback, early_stop_callback,\n",
    "        TQDMProgressBar(refresh_rate=10), lr_monitor\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "try:\n",
    "    trainer.fit(model)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training has been stopped manually.\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# save the final model\n",
    "#model.model.save_pretrained(os.path.join(args.output_dir, \"final\"))\n",
    "#tokenizer.save_pretrained(os.path.join(args.output_dir, \"final\"))\n",
    "print(\"Finish training and saving the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9abb8074-daf0-4b33-9f14-50000a493925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Dataset\n",
      "Total examples = 587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587 2935 2935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute scores given the predictions and gold labels\n",
    "\"\"\"\n",
    "\n",
    "outputs, targets, probs = [], [], []\n",
    "num_path = args.num_path\n",
    "\n",
    "if args.task in ['aste', 'tasd']:\n",
    "    num_path = min(5, num_path)\n",
    "    \n",
    "dataset = ABSADataset(tokenizer=model.tokenizer,\n",
    "                              task_name=args.task,\n",
    "                              data_setting=args.data_setting,\n",
    "                                data_name=args.dataset,\n",
    "                                language=args.lang,\n",
    "                                data_type='test',\n",
    "                              top_k=args.num_path,\n",
    "                              args=args,\n",
    "                              max_len=args.max_seq_length)\n",
    "\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=args.eval_batch_size,\n",
    "                         num_workers=2)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model.model.to(device)\n",
    "model.model.eval()\n",
    "\"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b597ff18-e844-49d6-bfa1-55ec1ebb1a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [02:33<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "args.constrained_decode = True\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    # beam search\n",
    "\n",
    "    outs = model.model.generate(\n",
    "        input_ids=batch['source_ids'].to(device),\n",
    "        attention_mask=batch['source_mask'].to(device),\n",
    "        max_length=args.max_seq_length,\n",
    "        num_beams=args.beam_size,\n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        prefix_allowed_tokens_fn=partial(\n",
    "           prefix_allowed_tokens_fn, model, args.task, args.dataset,\n",
    "           batch['source_ids']) if args.constrained_decode else None,\n",
    "    )\n",
    "    dec = [\n",
    "        model.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in outs.sequences\n",
    "    ]\n",
    "    target = [\n",
    "        model.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in batch[\"target_ids\"]\n",
    "    ]\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c220c2c0-b695-4cff-b1be-426e3f6e0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVP F1-Micro:  17.14865550481989\n"
     ]
    }
   ],
   "source": [
    "def compute_scores(pred_seqs, gold_seqs, verbose=True, task=\"asqp\"):\n",
    "    global all_labels\n",
    "    global all_preds\n",
    "    \"\"\"\n",
    "    Compute model performance\n",
    "    \"\"\"\n",
    "    assert len(pred_seqs) == len(gold_seqs), (len(pred_seqs), len(gold_seqs))\n",
    "    num_samples = len(gold_seqs)\n",
    "\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        gold_list = extract_spans_para(gold_seqs[i], 'gold')\n",
    "        pred_list = extract_spans_para(pred_seqs[i], 'pred')\n",
    "        if (task == \"tasd\"):\n",
    "          gold_list = [tup[:-1] for tup in gold_list]\n",
    "          pred_list = [tup[:-1] for tup in pred_list]\n",
    "\n",
    "        all_labels.append(gold_list)\n",
    "        all_preds.append(pred_list)\n",
    "\n",
    "    try:\n",
    "        preds = [[f'{labels[0]}:{opinion2sentword[labels[2]].upper() if labels[2] in opinion2sentword else \"\"}:{labels[1]}' for labels in pred] for pred in all_preds]\n",
    "        golds = [[f'{labels[0]}:{opinion2sentword[labels[2]].upper() if labels[2] in opinion2sentword else \"\"}:{labels[1]}' for labels in gold] for gold in all_labels]\n",
    "    except KeyError:\n",
    "        print('KeyError!')\n",
    "        print(all_labels)\n",
    "    except:\n",
    "        print(all_labels)\n",
    "\n",
    "    all_label = all_labels\n",
    "    \n",
    "    scores_dfs = createResults(preds, golds, LABEL_SPACE, task)\n",
    "    \n",
    "    scores = compute_f1_scores(all_preds, all_labels)\n",
    "    scores[\"all_preds\"] = all_preds\n",
    "    scores[\"all_labels\"] = all_labels\n",
    "    print('MVP F1-Micro: ', scores['f1'])\n",
    "    # return scores, all_labels, all_preds\n",
    "    return scores_dfs, all_labels, pred_seqs\n",
    "\n",
    "a = compute_scores(outputs, targets, verbose=True, task=args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d6606595-1c99-43d9-8507-f83d950046e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Prediction\n",
      "Preds\n",
      "['[A] null [S] great [C] restaurant general', '[A] sushi [S] great [C] food quality', '[A] portions [S] ok [C] food style_options', '[A] green tea creme brulee [S] great [C] food quality', '[A] restaurant [S] bad [C] restaurant general']\n",
      "Golds\n",
      "['[S] great [A] it [C] food quality', '[S] great [A] sushi [C] food quality', '[S] ok [A] portions [C] food style_options', '[S] great [A] green tea creme brulee [C] food quality', '[S] great [A] it [C] food quality']\n",
      "MVP F1-Micro:  56.39781685870224\n"
     ]
    }
   ],
   "source": [
    "targets = targets[::num_path]\n",
    "\n",
    "# get outputs\n",
    "_outputs = outputs # backup\n",
    "outputs = [] # new outputs\n",
    "if args.agg_strategy == 'post_rank':\n",
    "    inputs = [ele for ele in sents for _ in range(num_path)]\n",
    "    assert len(_outputs) == len(inputs), (len(_outputs), len(inputs))\n",
    "    preds = [[o] for o in _outputs] \n",
    "    model_path = os.path.join(args.output_dir, \"final\")\n",
    "    scores = cal_entropy(inputs, preds, model_path, model.tokenizer)\n",
    "\n",
    "for i in range(0, len(targets)):\n",
    "    o_idx = i * num_path\n",
    "    multi_outputs = _outputs[o_idx:o_idx + num_path]\n",
    "\n",
    "    if args.agg_strategy == 'post_rank':\n",
    "        multi_probs = scores[o_idx:o_idx + args.num_path]\n",
    "        assert len(multi_outputs) == len(multi_probs)\n",
    "\n",
    "        sorted_outputs = [i for _,i in sorted(zip(multi_probs,multi_outputs))]\n",
    "        outputs.append(sorted_outputs[0])\n",
    "        continue\n",
    "    elif args.agg_strategy == \"pre_rank\":\n",
    "        outputs.append(multi_outputs[0])\n",
    "        continue\n",
    "    elif args.agg_strategy == 'rand':\n",
    "        outputs.append(random.choice(multi_outputs))\n",
    "        continue\n",
    "    elif args.agg_strategy == 'vote':\n",
    "        all_quads = []\n",
    "        for s in multi_outputs:\n",
    "            all_quads.extend(\n",
    "                extract_spans_para(seq=s, seq_type='pred'))\n",
    "\n",
    "        output_quads = []\n",
    "        counter = dict(Counter(all_quads))\n",
    "        for quad, count in counter.items():\n",
    "            # keep freq >= num_path / 2\n",
    "            if count >= len(multi_outputs) / 2:\n",
    "                output_quads.append(quad)\n",
    "\n",
    "        # recover output\n",
    "        output = []\n",
    "        for q in output_quads:\n",
    "            ac, at, sp, ot = q\n",
    "            if args.task == \"aste\":\n",
    "                if 'null' not in [at, ot, sp]:  # aste has no 'null', for zero-shot only\n",
    "                    output.append(f'[A] {at} [O] {ot} [S] {sp}')\n",
    "\n",
    "            elif args.task  == \"tasd\":\n",
    "                output.append(f\"[A] {at} [S] {sp} [C] {ac}\")\n",
    "\n",
    "            elif args.task in [\"asqp\", \"acos\"]:\n",
    "                output.append(f\"[A] {at} [O] {ot} [S] {sp} [C] {ac}\")\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        target_quads = extract_spans_para(seq=targets[i],\n",
    "                                        seq_type='gold')\n",
    "\n",
    "        # if no output, use the first path\n",
    "        output_str = \" [SSEP] \".join(\n",
    "            output) if output else multi_outputs[0]\n",
    "\n",
    "        outputs.append(output_str)\n",
    "\n",
    "# stats\n",
    "labels_counts = Counter([len(l.split('[SSEP]')) for l in outputs])\n",
    "\n",
    "print('After Prediction')\n",
    "print('Preds')\n",
    "print(outputs[:5])\n",
    "print('Golds')\n",
    "print(targets[:5])\n",
    "\n",
    "scores, all_labels, preds = compute_scores(outputs,\n",
    "                                               targets,\n",
    "                                               verbose=True, task=args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "de1b6204-7123-4db8-b39e-fde85323fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ambience general': {'precision': 0.6538,\n",
       "  'recall': 0.7727,\n",
       "  'f1': 0.7083,\n",
       "  'accuracy': 0.5484,\n",
       "  'support': 93},\n",
       " 'drinks prices': {'precision': 0.3333,\n",
       "  'recall': 0.25,\n",
       "  'f1': 0.2857,\n",
       "  'accuracy': 0.1667,\n",
       "  'support': 6},\n",
       " 'drinks quality': {'precision': 0.8125,\n",
       "  'recall': 0.5909,\n",
       "  'f1': 0.6842,\n",
       "  'accuracy': 0.52,\n",
       "  'support': 25},\n",
       " 'drinks style_options': {'precision': 0.4615,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.48,\n",
       "  'accuracy': 0.3158,\n",
       "  'support': 19},\n",
       " 'food prices': {'precision': 0.4118,\n",
       "  'recall': 0.6087,\n",
       "  'f1': 0.4912,\n",
       "  'accuracy': 0.3256,\n",
       "  'support': 43},\n",
       " 'food quality': {'precision': 0.5651,\n",
       "  'recall': 0.4856,\n",
       "  'f1': 0.5223,\n",
       "  'accuracy': 0.3535,\n",
       "  'support': 430},\n",
       " 'food style_options': {'precision': 0.4419,\n",
       "  'recall': 0.3455,\n",
       "  'f1': 0.3878,\n",
       "  'accuracy': 0.2405,\n",
       "  'support': 79},\n",
       " 'location general': {'precision': 0.5455,\n",
       "  'recall': 0.4615,\n",
       "  'f1': 0.5,\n",
       "  'accuracy': 0.3333,\n",
       "  'support': 18},\n",
       " 'restaurant general': {'precision': 0.713,\n",
       "  'recall': 0.5775,\n",
       "  'f1': 0.6381,\n",
       "  'accuracy': 0.4686,\n",
       "  'support': 175},\n",
       " 'restaurant miscellaneous': {'precision': 0.2857,\n",
       "  'recall': 0.303,\n",
       "  'f1': 0.2941,\n",
       "  'accuracy': 0.1724,\n",
       "  'support': 58},\n",
       " 'restaurant prices': {'precision': 0.5,\n",
       "  'recall': 0.4286,\n",
       "  'f1': 0.4615,\n",
       "  'accuracy': 0.3,\n",
       "  'support': 30},\n",
       " 'service general': {'precision': 0.68,\n",
       "  'recall': 0.6581,\n",
       "  'f1': 0.6689,\n",
       "  'accuracy': 0.5025,\n",
       "  'support': 203},\n",
       " 'Micro-AVG': {'precision': 0.5886,\n",
       "  'recall': 0.5413,\n",
       "  'f1': 0.564,\n",
       "  'accuracy': 0.3927,\n",
       "  'support': 1184},\n",
       " 'Macro-AVG': {'precision': 0.5337,\n",
       "  'recall': 0.4985,\n",
       "  'f1': 0.5102,\n",
       "  'accuracy': '',\n",
       "  'support': 1184}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "178ad631-e641-45ea-a7ac-e02085e6d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_allowed_tokens_fn(model, task, data_name, source_ids, batch_id,\n",
    "                                 input_ids):\n",
    "    \"\"\"\n",
    "    Constrained Decoding\n",
    "    # ids = self.tokenizer(\"text\", return_tensors='pt')['input_ids'].tolist()[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    force_tokens = model.force_tokens\n",
    "    \n",
    "    # google/mt5-small\n",
    "    to_id = {\n",
    "        'OT': [646],\n",
    "        'AT': [357],\n",
    "        'SP': [399],\n",
    "        'AC': [424],\n",
    "        'SS': [399],\n",
    "        'EP': [155719],\n",
    "        '[': [491],\n",
    "        ']': [439],\n",
    "        'it': [609],\n",
    "        # 'es': [3, 15, 7],\n",
    "        'null': [259, 1181]\n",
    "    }\n",
    "\n",
    "    left_brace_index = (input_ids == to_id['['][0]).nonzero()\n",
    "    right_brace_index = (input_ids == to_id[']'][0]).nonzero()\n",
    "    num_left_brace = len(left_brace_index)\n",
    "    num_right_brace = len(right_brace_index)\n",
    "    last_right_brace_pos = right_brace_index[-1][\n",
    "        0] if right_brace_index.nelement() > 0 else -1\n",
    "    last_left_brace_pos = left_brace_index[-1][\n",
    "        0] if left_brace_index.nelement() > 0 else -1\n",
    "    cur_id = input_ids[-1]\n",
    "\n",
    "    if cur_id in to_id['[']:\n",
    "        return force_tokens['special_tokens']\n",
    "    elif cur_id in to_id['AT'] + to_id['OT'] + to_id['EP'] + to_id['SP'] + to_id['AC']:  \n",
    "        # return to_id[']']  \n",
    "        ### MT5 ONLY\n",
    "        return to_id[']'] + to_id['EP']  \n",
    "    elif cur_id in to_id['SS']:  \n",
    "        return to_id['EP'] \n",
    "   \n",
    "    # get cur_term\n",
    "    if last_left_brace_pos == -1:\n",
    "        return to_id['['] + [1]   # start of sentence: [\n",
    "    elif (last_left_brace_pos != -1 and last_right_brace_pos == -1) \\\n",
    "        or last_left_brace_pos > last_right_brace_pos:\n",
    "        return to_id[']']  # ]\n",
    "    else:\n",
    "        cur_term = input_ids[last_left_brace_pos + 1]\n",
    "\n",
    "    ret = []\n",
    "    if cur_term in to_id['SP']:  # SP\n",
    "        ret = force_tokens['sentiment_tokens'][str(task)]\n",
    "    elif cur_term in to_id['AT']:  # AT\n",
    "        force_list = source_ids[batch_id].tolist()\n",
    "        if task != 'aste': \n",
    "            if True:\n",
    "            # if data_name == 'rest-16':\n",
    "                force_list.extend(to_id['it'] + [1])  \n",
    "            # elif data_name == 'GERestaurant':\n",
    "            #     force_list.extend(to_id['es'] + [1])  \n",
    "        ret = force_list  \n",
    "    elif cur_term in to_id['SS']:\n",
    "        #ret = [3] + to_id[']'] + [1]\n",
    "        ### MT5 ONLY\n",
    "        ret = [259] + to_id[']'] + [1]\n",
    "    elif cur_term in to_id['AC']:  # AC\n",
    "        ret = force_tokens['cate_tokens'][str(data_name)]\n",
    "    elif cur_term in to_id['OT']:  # OT\n",
    "        force_list = source_ids[batch_id].tolist()\n",
    "        if task == \"acos\":\n",
    "            force_list.extend(to_id['null'])  # null\n",
    "        ret = force_list\n",
    "    else:\n",
    "        raise ValueError(cur_term)    \n",
    "\n",
    "    if num_left_brace == num_right_brace:\n",
    "        ret = set(ret)\n",
    "        ret.discard(to_id[']'][0]) # remove ]\n",
    "        for w in force_tokens['special_tokens']:\n",
    "            ret.discard(w)\n",
    "        ret = list(ret)\n",
    "    elif num_left_brace > num_right_brace:\n",
    "        ret += to_id[']'] \n",
    "    else:\n",
    "        raise ValueError\n",
    "    ret.extend(to_id['['] + [1]) # add [\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "14f89f2b-b4e9-4978-81d4-b92f4faef0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[S] bad [A] food  food [C] food quality [SSEP] [S] bad [A] food [C] food quality [SSEP] [C] food quality [SSEP] [C] food quality [SSEP] [C] food quality [A] food [S] bad [SSEP] [C] food quality [A] it'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = 'the lemon chicken tasted like sticky sweet donuts and the honey walnut prawns, the few they actually give you.....were not good.'\n",
    "\n",
    "tokenized_input = model.tokenizer.batch_encode_plus(\n",
    "                        [st],\n",
    "                        max_length=1000,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "outs1 = model.model.generate(\n",
    "        input_ids=tokenized_input['input_ids'].to(device),\n",
    "        attention_mask=tokenized_input['attention_mask'].to(device),\n",
    "        max_length=1000,\n",
    "        num_beams=args.beam_size,\n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        prefix_allowed_tokens_fn=partial(\n",
    "           prefix_allowed_tokens_fn, model, args.task, args.dataset,\n",
    "           batch['source_ids']) if args.constrained_decode else None,\n",
    ")\n",
    "\n",
    "model.tokenizer.decode(outs1.sequences[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "93578999-245f-45e9-824a-4432463265ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'food quality': 313, 'service general': 155, 'restaurant general': 142, 'ambience general': 66, 'food style_options': 55, 'restaurant miscellaneous': 33, 'food prices': 23, 'drinks quality': 22, 'restaurant prices': 21, 'location general': 13, 'drinks style_options': 12, 'drinks prices': 4})\n",
      "Counter({'food quality': 269, 'service general': 150, 'restaurant general': 115, 'ambience general': 78, 'food style_options': 43, 'restaurant miscellaneous': 35, 'food prices': 34, 'restaurant prices': 18, 'drinks quality': 16, 'drinks style_options': 13, 'location general': 11, 'null': 5, 'drinks prices': 3})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter([a  for labels in all_labels for a,at,p in labels]))\n",
    "print(Counter([a  for labels in all_preds for a,at,p in labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93e90851-4d81-4349-b2b4-fa1ac7683289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service general',\n",
       " 'food quality',\n",
       " 'restaurant general',\n",
       " 'food style_options',\n",
       " 'ambience general',\n",
       " 'location general',\n",
       " 'drinks quality',\n",
       " 'food prices',\n",
       " 'drinks style_options',\n",
       " 'drinks prices',\n",
       " 'restaurant prices',\n",
       " 'restaurant miscellaneous']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([a  for labels in all_labels for a,at,p in labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7495ca-23e7-4478-b605-de4949d8af99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
