{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ad87fa",
   "metadata": {},
   "source": [
    "### Update IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e608a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IDs by +555 and saved to MW_4_org.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- CONFIG ---\n",
    "name = \"MW_4_orgsd.jsonl\"\n",
    "INPUT_FILE = f\"ground_truth_tasd/{name}\"   # original file\n",
    "OUTPUT_FILE = f\"{name}\" # file to save updated IDs\n",
    "INCREMENT = 185*3              # value to add to each id\n",
    "\n",
    "# --- SCRIPT ---\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        obj = json.loads(line)\n",
    "        if \"id\" in obj and isinstance(obj[\"id\"], int):\n",
    "            obj[\"id\"] += INCREMENT\n",
    "        json.dump(obj, outfile, ensure_ascii=False)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Updated IDs by +{INCREMENT} and saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f3837",
   "metadata": {},
   "source": [
    "### Combine Single Annotations to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d45483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 batches\n",
      "Batch 1: ['MW_1.jsonl', 'ND_1.jsonl']\n",
      "Batch 2: ['MW_2.jsonl', 'ND_2.jsonl']\n",
      "Batch 3: ['MW_3.jsonl', 'ND_3.jsonl']\n",
      "Batch 4: ['MW_4.jsonl', 'ND_4.jsonl']\n",
      "Batch 5: ['MW_5.jsonl', 'ND_5.jsonl']\n",
      "Saved ground_truth_tasd_all\\combined_dataset_1.jsonl with 924 entries\n",
      "Saved ground_truth_tasd_all\\combined_dataset_2.jsonl with 924 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "FOLDER_PATH = \"ground_truth_tasd\"\n",
    "OUTPUT_FILE = FOLDER_PATH + \"_all\"\n",
    "\n",
    "os.makedirs(OUTPUT_FILE, exist_ok=True)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load a JSONL file as a list of dicts.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    \"\"\"Save a list of dicts to a JSONL file.\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List all JSONL files\n",
    "    all_files = sorted([f for f in os.listdir(FOLDER_PATH) if f.endswith(\".jsonl\")])\n",
    "    full_paths = [os.path.join(FOLDER_PATH, f) for f in all_files]\n",
    "\n",
    "    # Step 1: group files into batches (files with identical IDs)\n",
    "    file_ids = {}\n",
    "    batches = []\n",
    "    used_files = set()\n",
    "\n",
    "    for f in full_paths:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as jf:\n",
    "            ids = set(json.loads(line)[\"id\"] for line in jf)\n",
    "        file_ids[f] = ids\n",
    "\n",
    "    for f1 in full_paths:\n",
    "        if f1 in used_files:\n",
    "            continue\n",
    "        batch = [f1]\n",
    "        ids1 = file_ids[f1]\n",
    "        for f2 in full_paths:\n",
    "            if f2 != f1 and f2 not in used_files and file_ids[f2] == ids1:\n",
    "                batch.append(f2)\n",
    "        used_files.update(batch)\n",
    "        batches.append(sorted(batch))  # keep consistent order\n",
    "\n",
    "    print(f\"Detected {len(batches)} batches\")\n",
    "    for i, b in enumerate(batches):\n",
    "        print(f\"Batch {i+1}: {[os.path.basename(f) for f in b]}\")\n",
    "\n",
    "    # Step 2: combine files column-wise\n",
    "    num_datasets = len(batches[0])\n",
    "    for i in range(num_datasets):\n",
    "        combined_data = []\n",
    "        for batch in batches:\n",
    "            combined_data.extend(load_jsonl(batch[i]))\n",
    "        out_file = os.path.join(OUTPUT_FILE, f\"combined_dataset_{i+1}.jsonl\")\n",
    "        save_jsonl(combined_data, out_file)\n",
    "        print(f\"Saved {out_file} with {len(combined_data)} entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87547a0c",
   "metadata": {},
   "source": [
    "### Remove conflict and \\\\n in phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b00ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned MW_1.jsonl → AA_Output\\MW_1.jsonl\n",
      "Cleaned MW_2.jsonl → AA_Output\\MW_2.jsonl\n",
      "Cleaned MW_3.jsonl → AA_Output\\MW_3.jsonl\n",
      "Cleaned MW_4.jsonl → AA_Output\\MW_4.jsonl\n",
      "Cleaned MW_5.jsonl → AA_Output\\MW_5.jsonl\n",
      "Cleaned ND_1.jsonl → AA_Output\\ND_1.jsonl\n",
      "Cleaned ND_2.jsonl → AA_Output\\ND_2.jsonl\n",
      "Cleaned ND_3.jsonl → AA_Output\\ND_3.jsonl\n",
      "Cleaned ND_4.jsonl → AA_Output\\ND_4.jsonl\n",
      "Cleaned ND_5.jsonl → AA_Output\\ND_5.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "FOLDER_PATH = \"AA_Input\"\n",
    "OUTPUT_PATH = \"AA_Output\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Collapse all whitespace (\\n, \\t, multiple spaces) into a single space.\"\"\"\n",
    "    return s.replace(\"\\\\n\", \"\").strip()\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \"\"\"Remove conflict labels and normalize phrases if present.\"\"\"\n",
    "    cleaned = []\n",
    "    for label in labels:\n",
    "        # skip conflict labels\n",
    "        if len(label) >= 2 and label[1].lower() == \"conflict\":\n",
    "            continue\n",
    "        # normalize phrase if TASD (triplet)\n",
    "        if len(label) == 3:\n",
    "            cat, pol, phrase = label\n",
    "            cleaned.append([cat, pol, normalize(phrase)])\n",
    "        else:\n",
    "            cleaned.append(label)\n",
    "    return cleaned\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_files = sorted([f for f in os.listdir(FOLDER_PATH) if f.endswith(\".jsonl\")])\n",
    "\n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(FOLDER_PATH, file)\n",
    "        entries = load_jsonl(file_path)\n",
    "\n",
    "        for entry in entries:\n",
    "            entry[\"labels\"] = clean_labels(entry[\"labels\"])\n",
    "\n",
    "        out_file = os.path.join(OUTPUT_PATH, file)\n",
    "        save_jsonl(entries, out_file)\n",
    "        print(f\"Cleaned {file} → {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59869f72",
   "metadata": {},
   "source": [
    "### Update category / polarity and remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68d1010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed MW_1.jsonl → AA_Output\\MW_1.jsonl\n",
      "Transformed MW_2.jsonl → AA_Output\\MW_2.jsonl\n",
      "Transformed MW_3.jsonl → AA_Output\\MW_3.jsonl\n",
      "Transformed MW_4.jsonl → AA_Output\\MW_4.jsonl\n",
      "Transformed MW_5.jsonl → AA_Output\\MW_5.jsonl\n",
      "Transformed ND_1.jsonl → AA_Output\\ND_1.jsonl\n",
      "Transformed ND_2.jsonl → AA_Output\\ND_2.jsonl\n",
      "Transformed ND_3.jsonl → AA_Output\\ND_3.jsonl\n",
      "Transformed ND_4.jsonl → AA_Output\\ND_4.jsonl\n",
      "Transformed ND_5.jsonl → AA_Output\\ND_5.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "FOLDER_PATH = \"AA_Input\"\n",
    "OUTPUT_PATH = \"AA_Output\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Mapping from German polarities to English\n",
    "GERMAN_TO_EN_POLARITY = {\n",
    "    \"positiv\": \"positive\",\n",
    "    \"negativ\": \"negative\",\n",
    "    \"neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def transform_labels(labels):\n",
    "    \"\"\"Lowercase category and map polarity from German to English.\"\"\"\n",
    "    transformed = []\n",
    "    for label in labels:\n",
    "        if len(label) == 2:\n",
    "            cat, pol = label\n",
    "            cat = cat.lower()\n",
    "            pol = GERMAN_TO_EN_POLARITY.get(pol.lower(), pol.lower())\n",
    "            transformed.append([cat, pol])\n",
    "        elif len(label) == 3:\n",
    "            cat, pol, phrase = label\n",
    "            cat = cat.lower()\n",
    "            pol = GERMAN_TO_EN_POLARITY.get(pol.lower(), pol.lower())\n",
    "            transformed.append([cat, pol, phrase])\n",
    "        else:\n",
    "            transformed.append(label)  # leave as is if unexpected\n",
    "    return transformed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_files = sorted([f for f in os.listdir(FOLDER_PATH) if f.endswith(\".jsonl\")])\n",
    "\n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(FOLDER_PATH, file)\n",
    "        entries = load_jsonl(file_path)\n",
    "\n",
    "        for entry in entries:\n",
    "            entry[\"labels\"] = transform_labels(entry[\"labels\"])\n",
    "\n",
    "        out_file = os.path.join(OUTPUT_PATH, file)\n",
    "        save_jsonl(entries, out_file)\n",
    "        print(f\"Transformed {file} → {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
